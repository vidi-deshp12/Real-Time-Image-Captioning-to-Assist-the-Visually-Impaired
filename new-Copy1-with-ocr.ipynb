{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c0fddd-eeb8-4c9f-8b10-a39befc8865d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aa4f360-786c-4763-a4e2-44c40e154b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\vdsha\\CLIP\")\n",
    "from clip import clip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28af6683-f365-48e7-95dc-ca27241d65e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdsha\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from clip import clip\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load CLIP model and preprocessing pipeline\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d2bab01-8668-4b8f-9e1f-54518bb02413",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Model\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2LMHeadModel\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2LMHeadModel\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "#@title Model\n",
    "T = torch.Tensor\n",
    "D = torch.device\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def forward(self, x: T) -> T:\n",
    "        return self.model(x)\n",
    "\n",
    "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(sizes) -1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(act())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "\n",
    "    #@functools.lru_cache #FIXME\n",
    "    def get_dummy_token(self, batch_size: int, device: D) -> T:\n",
    "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
    "\n",
    "    def forward(self, tokens: T, prefix: T, mask: Optional[T] = None, labels: Optional[T] = None):\n",
    "        embedding_text = self.gpt.transformer.wte(tokens)\n",
    "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
    "        \n",
    "        #print(embedding_text.size()) #torch.Size([5, 67, 768])\n",
    "        #print(prefix_projections.size()) #torch.Size([5, 1, 768])\n",
    "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
    "        if labels is not None:\n",
    "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
    "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
    "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
    "        return out\n",
    "\n",
    "    def __init__(self, prefix_length: int, prefix_size: int = 512):\n",
    "        super(ClipCaptionModel, self).__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "        if prefix_length > 10:  # not enough memory\n",
    "            self.clip_project = nn.Linear(prefix_size, self.gpt_embedding_size * prefix_length)\n",
    "        else:\n",
    "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n",
    "\n",
    "\n",
    "class ClipCaptionPrefix(ClipCaptionModel):\n",
    "\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return self.clip_project.parameters()\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        super(ClipCaptionPrefix, self).train(mode)\n",
    "        self.gpt.eval()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7416104-1249-4725-abea-56b196ec61d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50f3990a-e48f-4b89-a426-4fadbc68d1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_path = {'train':r\"D:\\vidisha\\vizwiz\\annotations\\train.json\"}\n",
    "images_path = {'train':r\"D:\\vidisha\\vizwiz\\train\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c71be1c-bc74-44dc-b7c4-2579d3f17f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'dict'>\n",
      "First key: info\n",
      "First value: {'description': 'This dataset contains crowdsourced captions of images from VizWiz datasets. This file contains the train partition.', 'license': {'url': 'https://creativecommons.org/licenses/by/4.0/', 'name': 'Attribution 4.0 International (CC BY 4.0)'}, 'url': 'https://vizwiz.org', 'version': 'VizWiz-Captions 1.0', 'year': 2019, 'contributor': 'VizWiz-Captions Consortium', 'date_created': '2019-12-23'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to the annotations JSON file\n",
    "annotation_file = annotations_path['train']\n",
    "\n",
    "# Load the JSON data\n",
    "with open(annotation_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Check the type of the loaded data\n",
    "print(f\"Data type: {type(data)}\")\n",
    "\n",
    "# If it's a list, print the first element to inspect its structure\n",
    "if isinstance(data, list):\n",
    "    print(f\"First element in the list: {data[0]}\")\n",
    "elif isinstance(data, dict):\n",
    "    # If it's a dictionary, print the first key-value pair\n",
    "    first_key = next(iter(data))\n",
    "    print(f\"First key: {first_key}\")\n",
    "    print(f\"First value: {data[first_key]}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Unexpected data format: {type(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "125b5cfd-93a0-46ef-a11c-0c911b89f35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'images', 'annotations'])\n"
     ]
    }
   ],
   "source": [
    "# Inspect the keys in the data dictionary to find where the captions are stored\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc6a3bff-113f-4b94-a849-e3c1b54033b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'caption': 'ITS IS A BASIL LEAVES CONTAINER ITS CONTAINS THE NET WEIGHT TOO.', 'image_id': 0, 'is_precanned': False, 'is_rejected': False, 'id': 0, 'text_detected': True}, {'caption': 'A green and white plastic condiment bottle containing Basil leaves.', 'image_id': 0, 'is_precanned': False, 'is_rejected': False, 'id': 1, 'text_detected': True}, {'caption': 'Quality issues are too severe to recognize visual content.', 'image_id': 0, 'is_precanned': True, 'is_rejected': True, 'id': 2, 'text_detected': True}, {'caption': 'A bottle of spices in a plastic container laying on a surface.', 'image_id': 0, 'is_precanned': False, 'is_rejected': False, 'id': 3, 'text_detected': True}, {'caption': 'some basil leaves in a container on a counter', 'image_id': 0, 'is_precanned': False, 'is_rejected': False, 'id': 4, 'text_detected': True}]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the structure of the annotations\n",
    "print(data['annotations'][:5])  # Print the first 5 entries to get an idea of the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ec96aaa-611b-4bb7-aeec-884fc10039b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>image_id</th>\n",
       "      <th>is_precanned</th>\n",
       "      <th>is_rejected</th>\n",
       "      <th>id</th>\n",
       "      <th>text_detected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ITS IS A BASIL LEAVES CONTAINER ITS CONTAINS T...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A green and white plastic condiment bottle con...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quality issues are too severe to recognize vis...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A bottle of spices in a plastic container layi...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>some basil leaves in a container on a counter</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A can of Coca Cola on a counter is shown for w...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A black can of Coca Cola Zero calorie soda is ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A kitchen counter the various items on top inc...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a black tin of Coca Cola placed on a black sur...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Black counter with canisters, kettle and can o...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             caption  image_id  is_precanned  \\\n",
       "0  ITS IS A BASIL LEAVES CONTAINER ITS CONTAINS T...         0         False   \n",
       "1  A green and white plastic condiment bottle con...         0         False   \n",
       "2  Quality issues are too severe to recognize vis...         0          True   \n",
       "3  A bottle of spices in a plastic container layi...         0         False   \n",
       "4      some basil leaves in a container on a counter         0         False   \n",
       "5  A can of Coca Cola on a counter is shown for w...         1         False   \n",
       "6  A black can of Coca Cola Zero calorie soda is ...         1         False   \n",
       "7  A kitchen counter the various items on top inc...         1         False   \n",
       "8  a black tin of Coca Cola placed on a black sur...         1         False   \n",
       "9  Black counter with canisters, kettle and can o...         1         False   \n",
       "\n",
       "   is_rejected  id  text_detected  \n",
       "0        False   0           True  \n",
       "1        False   1           True  \n",
       "2         True   2           True  \n",
       "3        False   3           True  \n",
       "4        False   4           True  \n",
       "5        False   5           True  \n",
       "6        False   6           True  \n",
       "7        False   7           True  \n",
       "8        False   8           True  \n",
       "9        False   9           True  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract the annotations from the data\n",
    "annotations = data['annotations']\n",
    "\n",
    "# Create the DataFrame from the annotations list\n",
    "df = pd.DataFrame(annotations)\n",
    "\n",
    "# Optionally, reset the index and drop the 'index' column if you need to\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa2fd12e-8811-4f18-845a-bede3825cc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of captions: 39994\n",
      "Number of image paths: 39994\n",
      "D:\\vidisha\\vizwiz\\train\\VizWiz_train_00000000.jpg ['<start> ITS IS A BASIL LEAVES CONTAINER ITS CONTAINS THE NET WEIGHT TOO. <end>', '<start> A green and white plastic condiment bottle containing Basil leaves. <end>', '<start> Quality issues are too severe to recognize visual content. <end>', '<start> A bottle of spices in a plastic container laying on a surface. <end>', '<start> some basil leaves in a container on a counter <end>']\n",
      "D:\\vidisha\\vizwiz\\train\\VizWiz_train_00000001.jpg ['<start> A can of Coca Cola on a counter is shown for when one can use a nice, cold drink. <end>', '<start> A black can of Coca Cola Zero calorie soda is on the counter near the coffee maker. <end>', '<start> A kitchen counter the various items on top including a can of Coca-Cola, metal containers, and a teapot. <end>', '<start> a black tin of Coca Cola placed on a black surface <end>', '<start> Black counter with canisters, kettle and can of soda. <end>']\n",
      "D:\\vidisha\\vizwiz\\train\\VizWiz_train_00000002.jpg ['<start> A can of crushed tomatoes are on a brown surface, the tomatoes read crushed tomatoes on the brand. <end>', '<start> A can of crushed tomatoes sitting on a beige colored counter. <end>', '<start> a can of crushed tomatoes in puree from price chopper. <end>', '<start> a Price Chopper branded can of crushed tomatoes <end>', '<start> Image is a can of crushed tomatoes in view. <end>']\n",
      "D:\\vidisha\\vizwiz\\train\\VizWiz_train_00000003.jpg ['<start> A white screen with a captcha that needs to be completed. <end>', '<start> A screenshot of a Captcha code on a phone screen. <end>', '<start> Screenshot from a smartphone with a case insensitive security measure. <end>', '<start> image shows a screenshot of a page required captcha. <end>', '<start> A screenshot of Spotify page on a cell phone saying click here to start download, showing a captcha that says T36M (case sensitive). <end>']\n",
      "D:\\vidisha\\vizwiz\\train\\VizWiz_train_00000004.jpg [\"<start> A box for a garden light rests in someone's lap. <end>\", '<start> A box containing information about a solar garden light product <end>', \"<start> A garden book is sitting on a person's lap. <end>\", \"<start> a box for a solar garden light laying on someone's lap <end>\", '<start> A blue and yellow box with lights for the garden inside. <end>']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import collections\n",
    "\n",
    "# Initialize the defaultdict\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "\n",
    "# Limit to the first 20,000 rows\n",
    "limit = 40000\n",
    "\n",
    "# Iterate over the rows of the DataFrame\n",
    "for n in range(min(df.shape[0], limit)):\n",
    "    # image_path = os.path.abspath('.') + images_path['train'] + str(df.iloc[n]['image_id']) + '.jpg'  # Assuming .jpg format\n",
    "    image_path = os.path.join(\n",
    "    images_path['train'], \n",
    "    f\"VizWiz_train_{df.iloc[n]['image_id']:08d}.jpg\"  # Ensures zero-padding to 8 digits\n",
    "    )\n",
    "    caption = f'<start> {df.iloc[n][\"caption\"]} <end>'\n",
    "    if len(caption) > 300:\n",
    "        continue\n",
    "    image_path_to_caption[image_path].append(caption)\n",
    "\n",
    "# Convert to lists as before\n",
    "captions = []\n",
    "img_name_vector = []\n",
    "for image_path, caption_list in image_path_to_caption.items():\n",
    "    captions.extend(caption_list)\n",
    "    img_name_vector.extend([image_path] * len(caption_list))\n",
    "\n",
    "# print(\"First 5 captions:\", captions[:5])\n",
    "# print(\"First 5 image paths:\", img_name_vector[:5])\n",
    "\n",
    "num_captions = len(captions)\n",
    "num_images = len(img_name_vector)\n",
    "\n",
    "print(f\"Number of captions: {num_captions}\")\n",
    "print(f\"Number of image paths: {num_images}\")\n",
    "\n",
    "# Print the first 5 entries of the dictionary for verification\n",
    "for i, (image_path, captions) in enumerate(image_path_to_caption.items()):\n",
    "    if i < 5:  # Limiting to first 5 entries\n",
    "        print(image_path, captions)\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60ca16bb-7eb1-4447-a5ce-2f4215880835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption sizes and their frequencies:\n",
      "Size 78: 621 captions\n",
      "Size 81: 564 captions\n",
      "Size 72: 5801 captions\n",
      "Size 76: 794 captions\n",
      "Size 59: 913 captions\n",
      "Size 95: 244 captions\n",
      "Size 97: 215 captions\n",
      "Size 118: 87 captions\n",
      "Size 64: 1093 captions\n",
      "Size 67: 1010 captions\n",
      "Size 112: 95 captions\n",
      "Size 75: 869 captions\n",
      "Size 68: 925 captions\n",
      "Size 61: 998 captions\n",
      "Size 57: 862 captions\n",
      "Size 71: 922 captions\n",
      "Size 63: 1080 captions\n",
      "Size 84: 403 captions\n",
      "Size 66: 988 captions\n",
      "Size 146: 28 captions\n",
      "Size 62: 1010 captions\n",
      "Size 77: 660 captions\n",
      "Size 70: 934 captions\n",
      "Size 91: 298 captions\n",
      "Size 69: 955 captions\n",
      "Size 83: 489 captions\n",
      "Size 58: 818 captions\n",
      "Size 65: 1088 captions\n",
      "Size 53: 460 captions\n",
      "Size 50: 258 captions\n",
      "Size 94: 233 captions\n",
      "Size 51: 294 captions\n",
      "Size 54: 572 captions\n",
      "Size 49: 174 captions\n",
      "Size 73: 838 captions\n",
      "Size 89: 337 captions\n",
      "Size 82: 472 captions\n",
      "Size 92: 285 captions\n",
      "Size 60: 992 captions\n",
      "Size 88: 352 captions\n",
      "Size 79: 653 captions\n",
      "Size 90: 294 captions\n",
      "Size 104: 155 captions\n",
      "Size 96: 223 captions\n",
      "Size 243: 1 captions\n",
      "Size 127: 49 captions\n",
      "Size 85: 395 captions\n",
      "Size 138: 24 captions\n",
      "Size 121: 67 captions\n",
      "Size 108: 133 captions\n",
      "Size 110: 96 captions\n",
      "Size 129: 43 captions\n",
      "Size 52: 362 captions\n",
      "Size 166: 10 captions\n",
      "Size 86: 383 captions\n",
      "Size 56: 779 captions\n",
      "Size 55: 601 captions\n",
      "Size 100: 177 captions\n",
      "Size 107: 129 captions\n",
      "Size 80: 552 captions\n",
      "Size 139: 26 captions\n",
      "Size 93: 239 captions\n",
      "Size 114: 83 captions\n",
      "Size 218: 5 captions\n",
      "Size 214: 5 captions\n",
      "Size 113: 117 captions\n",
      "Size 74: 787 captions\n",
      "Size 99: 188 captions\n",
      "Size 47: 73 captions\n",
      "Size 87: 358 captions\n",
      "Size 141: 18 captions\n",
      "Size 103: 152 captions\n",
      "Size 123: 55 captions\n",
      "Size 126: 47 captions\n",
      "Size 120: 64 captions\n",
      "Size 109: 100 captions\n",
      "Size 106: 115 captions\n",
      "Size 119: 70 captions\n",
      "Size 134: 37 captions\n",
      "Size 115: 84 captions\n",
      "Size 125: 67 captions\n",
      "Size 160: 14 captions\n",
      "Size 105: 140 captions\n",
      "Size 101: 179 captions\n",
      "Size 117: 83 captions\n",
      "Size 124: 60 captions\n",
      "Size 131: 44 captions\n",
      "Size 128: 37 captions\n",
      "Size 122: 60 captions\n",
      "Size 111: 101 captions\n",
      "Size 140: 27 captions\n",
      "Size 137: 28 captions\n",
      "Size 102: 164 captions\n",
      "Size 98: 167 captions\n",
      "Size 135: 29 captions\n",
      "Size 48: 109 captions\n",
      "Size 152: 18 captions\n",
      "Size 148: 20 captions\n",
      "Size 142: 17 captions\n",
      "Size 132: 36 captions\n",
      "Size 162: 11 captions\n",
      "Size 149: 15 captions\n",
      "Size 43: 10 captions\n",
      "Size 157: 12 captions\n",
      "Size 199: 3 captions\n",
      "Size 46: 59 captions\n",
      "Size 167: 10 captions\n",
      "Size 177: 7 captions\n",
      "Size 130: 33 captions\n",
      "Size 178: 10 captions\n",
      "Size 116: 68 captions\n",
      "Size 205: 3 captions\n",
      "Size 153: 14 captions\n",
      "Size 154: 11 captions\n",
      "Size 200: 5 captions\n",
      "Size 44: 12 captions\n",
      "Size 171: 3 captions\n",
      "Size 136: 30 captions\n",
      "Size 145: 14 captions\n",
      "Size 133: 29 captions\n",
      "Size 156: 16 captions\n",
      "Size 183: 7 captions\n",
      "Size 144: 21 captions\n",
      "Size 221: 2 captions\n",
      "Size 169: 13 captions\n",
      "Size 219: 2 captions\n",
      "Size 159: 12 captions\n",
      "Size 232: 2 captions\n",
      "Size 158: 16 captions\n",
      "Size 143: 24 captions\n",
      "Size 293: 1 captions\n",
      "Size 175: 11 captions\n",
      "Size 248: 1 captions\n",
      "Size 163: 10 captions\n",
      "Size 249: 2 captions\n",
      "Size 201: 1 captions\n",
      "Size 226: 3 captions\n",
      "Size 147: 18 captions\n",
      "Size 233: 2 captions\n",
      "Size 41: 1 captions\n",
      "Size 45: 21 captions\n",
      "Size 151: 19 captions\n",
      "Size 165: 14 captions\n",
      "Size 155: 7 captions\n",
      "Size 161: 10 captions\n",
      "Size 187: 2 captions\n",
      "Size 176: 10 captions\n",
      "Size 238: 1 captions\n",
      "Size 266: 3 captions\n",
      "Size 231: 2 captions\n",
      "Size 196: 7 captions\n",
      "Size 150: 12 captions\n",
      "Size 203: 2 captions\n",
      "Size 186: 3 captions\n",
      "Size 174: 9 captions\n",
      "Size 189: 4 captions\n",
      "Size 182: 7 captions\n",
      "Size 198: 6 captions\n",
      "Size 247: 2 captions\n",
      "Size 191: 5 captions\n",
      "Size 262: 1 captions\n",
      "Size 190: 5 captions\n",
      "Size 179: 5 captions\n",
      "Size 240: 3 captions\n",
      "Size 235: 4 captions\n",
      "Size 185: 9 captions\n",
      "Size 170: 5 captions\n",
      "Size 168: 6 captions\n",
      "Size 265: 1 captions\n",
      "Size 250: 1 captions\n",
      "Size 181: 2 captions\n",
      "Size 188: 5 captions\n",
      "Size 164: 4 captions\n",
      "Size 42: 1 captions\n",
      "Size 257: 2 captions\n",
      "Size 241: 2 captions\n",
      "Size 193: 1 captions\n",
      "Size 197: 3 captions\n",
      "Size 173: 4 captions\n",
      "Size 220: 1 captions\n",
      "Size 216: 2 captions\n",
      "Size 172: 12 captions\n",
      "Size 254: 2 captions\n",
      "Size 206: 5 captions\n",
      "Size 269: 2 captions\n",
      "Size 275: 3 captions\n",
      "Size 180: 1 captions\n",
      "Size 217: 1 captions\n",
      "Size 253: 1 captions\n",
      "Size 223: 2 captions\n",
      "Size 184: 4 captions\n",
      "Size 192: 4 captions\n",
      "Size 212: 3 captions\n",
      "Size 213: 2 captions\n",
      "Size 242: 2 captions\n",
      "Size 209: 2 captions\n",
      "Size 282: 1 captions\n",
      "Size 236: 2 captions\n",
      "Size 208: 3 captions\n",
      "Size 286: 1 captions\n",
      "Size 274: 1 captions\n",
      "Size 288: 1 captions\n",
      "Size 211: 1 captions\n",
      "Size 264: 1 captions\n",
      "Size 270: 1 captions\n",
      "Size 268: 1 captions\n",
      "Size 229: 1 captions\n",
      "Size 234: 1 captions\n",
      "Size 37: 2 captions\n",
      "Size 245: 2 captions\n",
      "Size 225: 3 captions\n",
      "Size 281: 1 captions\n",
      "Size 194: 1 captions\n",
      "Size 272: 1 captions\n",
      "Size 227: 1 captions\n",
      "Size 279: 1 captions\n",
      "Size 228: 1 captions\n",
      "Size 224: 1 captions\n",
      "Size 204: 1 captions\n",
      "Size 210: 1 captions\n"
     ]
    }
   ],
   "source": [
    "# Now we separate the captions and image paths into two lists\n",
    "captions = []\n",
    "img_name_vector = []\n",
    "\n",
    "for image_path, caption_list in image_path_to_caption.items():\n",
    "    captions.extend(caption_list)\n",
    "    img_name_vector.extend([image_path] * len(caption_list))  # Add image path multiple times for each caption\n",
    "\n",
    "# Check the distribution of caption lengths\n",
    "sizes_to_indices = collections.defaultdict(list)\n",
    "for index, caption in enumerate(captions):\n",
    "    size = len(caption)\n",
    "    sizes_to_indices[size].append(index)\n",
    "\n",
    "# Inspect caption lengths and their distribution\n",
    "print(\"Caption sizes and their frequencies:\")\n",
    "for size, indices in sizes_to_indices.items():\n",
    "    print(f\"Size {size}: {len(indices)} captions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1c9d7af-37bf-4bb4-857e-04e4fdd0e159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[293,\n",
       " 288,\n",
       " 286,\n",
       " 282,\n",
       " 281,\n",
       " 279,\n",
       " 275,\n",
       " 274,\n",
       " 272,\n",
       " 270,\n",
       " 269,\n",
       " 268,\n",
       " 266,\n",
       " 265,\n",
       " 264,\n",
       " 262,\n",
       " 257,\n",
       " 254,\n",
       " 253,\n",
       " 250,\n",
       " 249,\n",
       " 248,\n",
       " 247,\n",
       " 245,\n",
       " 243,\n",
       " 242,\n",
       " 241,\n",
       " 240,\n",
       " 238,\n",
       " 236,\n",
       " 235,\n",
       " 234,\n",
       " 233,\n",
       " 232,\n",
       " 231,\n",
       " 229,\n",
       " 228,\n",
       " 227,\n",
       " 226,\n",
       " 225,\n",
       " 224,\n",
       " 223,\n",
       " 221,\n",
       " 220,\n",
       " 219,\n",
       " 218,\n",
       " 217,\n",
       " 216,\n",
       " 214,\n",
       " 213,\n",
       " 212,\n",
       " 211,\n",
       " 210,\n",
       " 209,\n",
       " 208,\n",
       " 206,\n",
       " 205,\n",
       " 204,\n",
       " 203,\n",
       " 201,\n",
       " 200,\n",
       " 199,\n",
       " 198,\n",
       " 197,\n",
       " 196,\n",
       " 194,\n",
       " 193,\n",
       " 192,\n",
       " 191,\n",
       " 190,\n",
       " 189,\n",
       " 188,\n",
       " 187,\n",
       " 186,\n",
       " 185,\n",
       " 184,\n",
       " 183,\n",
       " 182,\n",
       " 181,\n",
       " 180,\n",
       " 179,\n",
       " 178,\n",
       " 177,\n",
       " 176,\n",
       " 175,\n",
       " 174,\n",
       " 173,\n",
       " 172,\n",
       " 171,\n",
       " 170,\n",
       " 169,\n",
       " 168,\n",
       " 167,\n",
       " 166,\n",
       " 165,\n",
       " 164,\n",
       " 163,\n",
       " 162,\n",
       " 161,\n",
       " 160,\n",
       " 159,\n",
       " 158,\n",
       " 157,\n",
       " 156,\n",
       " 155,\n",
       " 154,\n",
       " 153,\n",
       " 152,\n",
       " 151,\n",
       " 150,\n",
       " 149,\n",
       " 148,\n",
       " 147,\n",
       " 146,\n",
       " 145,\n",
       " 144,\n",
       " 143,\n",
       " 142,\n",
       " 141,\n",
       " 140,\n",
       " 139,\n",
       " 138,\n",
       " 137,\n",
       " 136,\n",
       " 135,\n",
       " 134,\n",
       " 133,\n",
       " 132,\n",
       " 131,\n",
       " 130,\n",
       " 129,\n",
       " 128,\n",
       " 127,\n",
       " 126,\n",
       " 125,\n",
       " 124,\n",
       " 123,\n",
       " 122,\n",
       " 121,\n",
       " 120,\n",
       " 119,\n",
       " 118,\n",
       " 117,\n",
       " 116,\n",
       " 115,\n",
       " 114,\n",
       " 113,\n",
       " 112,\n",
       " 111,\n",
       " 110,\n",
       " 109,\n",
       " 108,\n",
       " 107,\n",
       " 106,\n",
       " 105,\n",
       " 104,\n",
       " 103,\n",
       " 102,\n",
       " 101,\n",
       " 100,\n",
       " 99,\n",
       " 98,\n",
       " 97,\n",
       " 96,\n",
       " 95,\n",
       " 94,\n",
       " 93,\n",
       " 92,\n",
       " 91,\n",
       " 90,\n",
       " 89,\n",
       " 88,\n",
       " 87,\n",
       " 86,\n",
       " 85,\n",
       " 84,\n",
       " 83,\n",
       " 82,\n",
       " 81,\n",
       " 80,\n",
       " 79,\n",
       " 78,\n",
       " 77,\n",
       " 76,\n",
       " 75,\n",
       " 74,\n",
       " 73,\n",
       " 72,\n",
       " 71,\n",
       " 70,\n",
       " 69,\n",
       " 68,\n",
       " 67,\n",
       " 66,\n",
       " 65,\n",
       " 64,\n",
       " 63,\n",
       " 62,\n",
       " 61,\n",
       " 60,\n",
       " 59,\n",
       " 58,\n",
       " 57,\n",
       " 56,\n",
       " 55,\n",
       " 54,\n",
       " 53,\n",
       " 52,\n",
       " 51,\n",
       " 50,\n",
       " 49,\n",
       " 48,\n",
       " 47,\n",
       " 46,\n",
       " 45,\n",
       " 44,\n",
       " 43,\n",
       " 42,\n",
       " 41,\n",
       " 37]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_and_images_validation = captions, img_name_vector\n",
    "\n",
    "sizes_to_indices_again = collections.defaultdict(list)\n",
    "for index, caption in enumerate(captions_and_images_validation[0]):\n",
    "  size = len(caption)\n",
    "  sizes_to_indices_again[size].append(index)\n",
    "sorted(list(sizes_to_indices_again.keys()),reverse=True)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6e79ac9-5840-4750-8477-ca33c1c9f2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39994\n"
     ]
    }
   ],
   "source": [
    "total =  0\n",
    "for key in sizes_to_indices.keys():\n",
    "    total += len(sizes_to_indices[key])\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20d93aec-52fd-4590-b45c-a425e5bf804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class VizWizDataset(Dataset):\n",
    "    def __init__(self, img_paths, captions, clip_model, preprocess, tokenizer, max_length=50):\n",
    "        self.img_paths = img_paths  # List of image paths\n",
    "        self.captions = captions  # List of captions corresponding to the images\n",
    "        self.clip_model = clip_model\n",
    "        self.preprocess = preprocess\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image path and caption at the specified index\n",
    "        image_path = self.img_paths[idx]\n",
    "        caption = self.captions[idx]\n",
    "\n",
    "        # Load and preprocess image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.preprocess(image)\n",
    "\n",
    "        # Tokenize caption\n",
    "        tokens = self.tokenizer(\n",
    "            caption,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return (image, tokens['input_ids'], tokens['attention_mask'])\n",
    "\n",
    "        # def __getitem__(self, idx):\n",
    "        #     image_path = self.img_paths[idx]   # Access image path from img_paths\n",
    "        #     caption = self.captions[idx]       # Access caption from captions\n",
    "    \n",
    "        #     # Load and preprocess image\n",
    "        #     image = Image.open(image_path).convert(\"RGB\")\n",
    "        #     image = self.preprocess(image)\n",
    "    \n",
    "        #     # Tokenize caption\n",
    "        #     tokens = self.tokenizer(\n",
    "        #         caption,\n",
    "        #         max_length=self.max_length,\n",
    "        #         padding=\"max_length\",\n",
    "        #         truncation=True,\n",
    "        #         return_tensors=\"pt\"\n",
    "        #     )\n",
    "    \n",
    "        #     # Return image tensor, tokenized input ids, and attention mask\n",
    "        #     return image, tokens['input_ids'].squeeze(0), tokens['attention_mask'].squeeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af6d46fd-7e18-48b0-959f-5f6a69eabc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Load GPT-2 tokenizer and add padding token\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token  # Set the padding token to be the same as EOS\n",
    "\n",
    "# # Add special tokens BEFORE training\n",
    "# tokenizer.add_special_tokens({'bos_token': '<start>', 'eos_token': '<end>', 'pad_token': '<pad>'})\n",
    "\n",
    "# # Explicitly set a unique ID for pad_token if it's overlapping with eos_token\n",
    "# tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(\"<pad>\")\n",
    "\n",
    "# gpt2_tokenizer.add_special_tokens({'bos_token': '<start>', 'eos_token': '<end>', 'pad_token': '<pad>'})\n",
    "# gpt2_tokenizer.pad_token_id = gpt2_tokenizer.convert_tokens_to_ids(\"<pad>\")\n",
    "\n",
    "\n",
    "# # Print token IDs to verify\n",
    "# print(\"BOS Token ID:\", gpt2_tokenizer.convert_tokens_to_ids(\"<start>\"))\n",
    "# print(\"EOS Token ID:\", gpt2_tokenizer.convert_tokens_to_ids(\"<end>\"))\n",
    "# print(\"PAD Token ID:\", gpt2_tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b3dcf41-2540-44f4-85e1-81f69de06329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 224, 224])\n",
      "Input IDs shape: torch.Size([1, 50])\n",
      "Attention Mask shape: torch.Size([1, 50])\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset\n",
    "dataset = VizWizDataset(\n",
    "    img_paths=img_name_vector,\n",
    "    captions=captions,\n",
    "    clip_model=clip_model,\n",
    "    preprocess=preprocess,\n",
    "    tokenizer=gpt2_tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "# Test a sample\n",
    "sample = dataset[0]\n",
    "print(\"Image shape:\", sample[0].shape)  # Image tensor\n",
    "print(\"Input IDs shape:\", sample[1].shape)  # Input IDs\n",
    "print(\"Attention Mask shape:\", sample[2].shape)  # Attention mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ba8e226-6cc3-432c-9748-fed17001d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Create DataLoader for your dataset\n",
    "# train_dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# # Test DataLoader with one batch\n",
    "# batch = next(iter(train_dataloader))\n",
    "\n",
    "# # Accessing elements of the batch\n",
    "# images, input_ids, attention_mask = batch\n",
    "\n",
    "# # Print shapes of the batch components\n",
    "# print(\"Images shape:\", images.shape)\n",
    "# print(\"Input IDs shape:\", input_ids.shape)\n",
    "# print(\"Attention Mask shape:\", attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "defffa25-8a16-4c03-892c-156fbec38dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test DataLoader with one batch\n",
    "# batch = next(iter(train_dataloader))\n",
    "\n",
    "# # Print the structure of the batch\n",
    "# print(f\"Batch type: {type(batch)}\")  # Should be a tuple or dictionary\n",
    "# if isinstance(batch, dict):\n",
    "#     print(f\"Batch keys: {batch.keys()}\")  # Print keys if the batch is a dictionary\n",
    "# elif isinstance(batch, tuple):\n",
    "#     print(f\"Batch length: {len(batch)}\")  # Print length if the batch is a tuple\n",
    "#     # Optionally print first few elements in the tuple to inspect\n",
    "#     for i, item in enumerate(batch):\n",
    "#         print(f\"Batch item {i}: {item.shape if isinstance(item, torch.Tensor) else 'Not a tensor'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1e52fb0-6fea-49dc-9bae-1f9c7c6c0c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch type: <class 'tuple'>\n",
      "Batch length: 3\n",
      "Batch item 0: torch.Size([16, 3, 224, 224])\n",
      "Batch item 1: torch.Size([16, 1, 50])\n",
      "Batch item 2: torch.Size([16, 1, 50])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define custom collate_fn to return tuple instead of list\n",
    "def collate_fn(batch):\n",
    "    images, input_ids, attention_masks = zip(*batch)  # Unzip the batch\n",
    "    images = torch.stack(images)  # Stack images into a batch tensor\n",
    "    input_ids = torch.stack(input_ids)  # Stack input_ids\n",
    "    attention_masks = torch.stack(attention_masks)  # Stack attention_masks\n",
    "    return images, input_ids, attention_masks\n",
    "\n",
    "# Now create the DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn , # Use the custom collate_fn\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Test the batch structure\n",
    "batch = next(iter(train_dataloader))\n",
    "print(f\"Batch type: {type(batch)}\")\n",
    "if isinstance(batch, tuple):\n",
    "    print(f\"Batch length: {len(batch)}\")\n",
    "    for i, item in enumerate(batch):\n",
    "        print(f\"Batch item {i}: {item.shape if isinstance(item, torch.Tensor) else 'Not a tensor'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec0b29-c685-492f-b2c2-9b3f42dcb61c",
   "metadata": {},
   "source": [
    "**Setting up validation dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3397fa5a-0be8-4676-8c7b-cab98a8e117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_path = {'train':r\"D:\\vidisha\\vizwiz\\annotations\\train.json\", 'val' :r\"D:\\vidisha\\vizwiz\\annotations\\val.json\"}\n",
    "images_path = {'train':r\"D:\\vidisha\\vizwiz\\train\", 'val' :r\"D:\\vidisha\\vizwiz\\val\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13a67500-989d-47ef-bd8f-ac4294e530e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'dict'>\n",
      "First key: info\n",
      "First value: {'description': 'This dataset contains crowdsourced captions of images from VizWiz datasets. This file contains the val partition.', 'license': {'url': 'https://creativecommons.org/licenses/by/4.0/', 'name': 'Attribution 4.0 International (CC BY 4.0)'}, 'url': 'https://vizwiz.org', 'version': 'VizWiz-Captions 1.0', 'year': 2019, 'contributor': 'VizWiz-Captions Consortium', 'date_created': '2019-12-23'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to the annotations JSON file\n",
    "annotation_file = annotations_path['val']\n",
    "\n",
    "# Load the JSON data\n",
    "with open(annotation_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Check the type of the loaded data\n",
    "print(f\"Data type: {type(data)}\")\n",
    "\n",
    "# If it's a list, print the first element to inspect its structure\n",
    "if isinstance(data, list):\n",
    "    print(f\"First element in the list: {data[0]}\")\n",
    "elif isinstance(data, dict):\n",
    "    # If it's a dictionary, print the first key-value pair\n",
    "    first_key = next(iter(data))\n",
    "    print(f\"First key: {first_key}\")\n",
    "    print(f\"First value: {data[first_key]}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Unexpected data format: {type(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "221c877f-6906-498c-be5d-736d528154cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'images', 'annotations'])\n"
     ]
    }
   ],
   "source": [
    "# Inspect the keys in the data dictionary to find where the captions are stored\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0724b7a6-b2b5-435a-8ec1-d23e521dda1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'caption': 'A computer screen shows a repair prompt on the screen.', 'image_id': 23431, 'is_precanned': False, 'is_rejected': False, 'id': 117155, 'text_detected': True}, {'caption': 'a computer screen with a repair automatically pop up', 'image_id': 23431, 'is_precanned': False, 'is_rejected': False, 'id': 117156, 'text_detected': True}, {'caption': 'partial computer screen showing the need of repairs', 'image_id': 23431, 'is_precanned': False, 'is_rejected': False, 'id': 117157, 'text_detected': True}, {'caption': 'Part of a computer monitor showing a computer repair message.', 'image_id': 23431, 'is_precanned': False, 'is_rejected': False, 'id': 117158, 'text_detected': True}, {'caption': 'The top of a laptop with a blue background and dark blue text.', 'image_id': 23431, 'is_precanned': False, 'is_rejected': False, 'id': 117159, 'text_detected': True}]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the structure of the annotations\n",
    "print(data['annotations'][:5])  # Print the first 5 entries to get an idea of the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db5591eb-5a3d-4118-b4f3-fe774175b565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'file_name': 'VizWiz_val_00000000.jpg', 'vizwiz_url': 'https://ivc.ischool.utexas.edu/VizWiz_visualization_img/VizWiz_val_00000000.jpg', 'id': 23431, 'text_detected': True}, {'file_name': 'VizWiz_val_00000001.jpg', 'vizwiz_url': 'https://ivc.ischool.utexas.edu/VizWiz_visualization_img/VizWiz_val_00000001.jpg', 'id': 23432, 'text_detected': True}, {'file_name': 'VizWiz_val_00000002.jpg', 'vizwiz_url': 'https://ivc.ischool.utexas.edu/VizWiz_visualization_img/VizWiz_val_00000002.jpg', 'id': 23433, 'text_detected': True}, {'file_name': 'VizWiz_val_00000003.jpg', 'vizwiz_url': 'https://ivc.ischool.utexas.edu/VizWiz_visualization_img/VizWiz_val_00000003.jpg', 'id': 23434, 'text_detected': True}, {'file_name': 'VizWiz_val_00000004.jpg', 'vizwiz_url': 'https://ivc.ischool.utexas.edu/VizWiz_visualization_img/VizWiz_val_00000004.jpg', 'id': 23435, 'text_detected': True}]\n"
     ]
    }
   ],
   "source": [
    "print(data['images'][:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cb4d91bb-0899-42f1-bd1d-01c9cfe52d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\vidisha\\vizwiz\\val\\VizWiz_val_00000000.jpg ['<start> A computer screen shows a repair prompt on the screen. <end>', '<start> a computer screen with a repair automatically pop up <end>', '<start> partial computer screen showing the need of repairs <end>', '<start> Part of a computer monitor showing a computer repair message. <end>', '<start> The top of a laptop with a blue background and dark blue text. <end>']\n",
      "D:\\vidisha\\vizwiz\\val\\VizWiz_val_00000001.jpg ['<start> A person is holding a bottle that has medicine for the night time. <end>', '<start> A bottle of medication has a white twist top. <end>', '<start> night time medication bottle being held by someone <end>', '<start> a person holding a small black bottle of NIGHT TIME <end>', '<start> A bottle of what appears to be cough syrup held in hand. <end>']\n",
      "D:\\vidisha\\vizwiz\\val\\VizWiz_val_00000002.jpg ['<start> a white paper showing an image of black and brown dog <end>', '<start> A library book with pictures of two dogs on the cover on a wooden table. <end>', '<start> A book with a black and a tan dog walking down a snowy street. <end>', '<start> The book cover shows two dogs in the snow <end>', '<start> A book cover title Dog Years with an image of a black and brown dog walking up the street, on the left side it has a due date sticker from a library. <end>']\n",
      "D:\\vidisha\\vizwiz\\val\\VizWiz_val_00000003.jpg ['<start> A white box is to the left of a blue box on a wooden table. <end>', '<start> A small rectangular red and white box next to a small rectangular blue box on a wooden surface. <end>', '<start> two boxes of  medicine, one white and red and the other blue sitting on a table <end>', '<start> Two boxes that appear to contain medication or eye drops <end>', '<start> Two boxes of pharmaceutical products left in a table <end>']\n",
      "D:\\vidisha\\vizwiz\\val\\VizWiz_val_00000004.jpg ['<start> close up of a computer monitor that is powered on. <end>', '<start> A monitor has a message displayed on it. <end>', '<start> Pictured here is a screenshot that shows an error message from an app. <end>', '<start> Computer screen displaying an error saying the display driver is not supported by Zoom Text. <end>', \"<start> a screenshot of someone's monitor that is having issues <end>\"]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "\n",
    "# Create a mapping of image_id to file_name from the 'images' data\n",
    "image_id_to_file_name = {img['id']: img['file_name'] for img in data['images']}\n",
    "\n",
    "# Extract the annotations from the data\n",
    "annotations = data['annotations']\n",
    "\n",
    "# Create the DataFrame from the annotations list\n",
    "df_val = pd.DataFrame(annotations)\n",
    "\n",
    "# Reset the index\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "\n",
    "# Initialize the defaultdict\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "\n",
    "# Limit to the first 40,000 rows (or adjust as necessary)\n",
    "limit = None\n",
    "\n",
    "# Iterate over the rows of the DataFrame\n",
    "for n in range(df_val.shape[0]):\n",
    "    # Get the file name from the mapping\n",
    "    file_name = image_id_to_file_name[df_val.iloc[n]['image_id']]\n",
    "    \n",
    "    # Construct the full image path\n",
    "    image_path = os.path.join(images_path['val'], file_name)\n",
    "    \n",
    "    # Add the caption with <start> and <end> tokens\n",
    "    caption = f\"<start> {df_val.iloc[n]['caption']} <end>\"\n",
    "    image_path_to_caption[image_path].append(caption)\n",
    "\n",
    "# Convert the dictionary to lists for further processing\n",
    "captions_val = []\n",
    "img_name_vector_val = []\n",
    "for image_path, caption_list in image_path_to_caption.items():\n",
    "    captions_val.extend(caption_list)\n",
    "    img_name_vector_val.extend([image_path] * len(caption_list))\n",
    "\n",
    "# Print the first 5 entries of the dictionary for verification\n",
    "for i, (image_path, captions) in enumerate(image_path_to_caption.items()):\n",
    "    if i < 5:  # Limiting to first 5 entries\n",
    "        print(image_path, captions)\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91e20c9e-f2db-4d37-aa5e-89e38fc547e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of captions: 38750\n",
      "Number of image paths: 38750\n"
     ]
    }
   ],
   "source": [
    "# Check the number of captions and image paths\n",
    "num_captions = len(captions_val)\n",
    "num_images = len(img_name_vector_val)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of captions: {num_captions}\")\n",
    "print(f\"Number of image paths: {num_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "90382575-3855-4647-b9fa-c6e49dda8775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dataset Size: 38750\n"
     ]
    }
   ],
   "source": [
    "# Create the validation Dataset\n",
    "val_dataset = VizWizDataset(\n",
    "    img_paths=img_name_vector_val,  # List of image paths for validation\n",
    "    captions=captions_val,  # List of captions for validation\n",
    "    clip_model=clip_model,  # Your CLIP model (or any other feature extractor you're using)\n",
    "    preprocess=preprocess,  # Your image preprocessing function\n",
    "    tokenizer=gpt2_tokenizer  # Your tokenizer (e.g., GPT-2 tokenizer)\n",
    ")\n",
    "\n",
    "# Check dataset length (just to verify)\n",
    "print(f\"Validation Dataset Size: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80265617-6fef-4af8-870f-f457a817f218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch type: <class 'tuple'>\n",
      "Batch length: 3\n",
      "Batch item 0: torch.Size([16, 3, 224, 224])\n",
      "Batch item 1: torch.Size([16, 1, 50])\n",
      "Batch item 2: torch.Size([16, 1, 50])\n"
     ]
    }
   ],
   "source": [
    "# Create the validation DataLoader\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,  # Adjust the batch size if needed\n",
    "    shuffle=False,  # No need to shuffle the validation data\n",
    "    collate_fn=collate_fn  # Leave as None unless you need to customize batching\n",
    ")\n",
    "\n",
    "# Test the DataLoader with one batch\n",
    "batch = next(iter(val_dataloader))\n",
    "\n",
    "# Print the structure of the batch\n",
    "print(f\"Batch type: {type(batch)}\")\n",
    "if isinstance(batch, tuple):\n",
    "    print(f\"Batch length: {len(batch)}\")\n",
    "    for i, item in enumerate(batch):\n",
    "        print(f\"Batch item {i}: {item.shape if isinstance(item, torch.Tensor) else 'Not a tensor'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "497a95e9-512c-4475-9a2d-e8096e24a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and optimizer\n",
    "prefix_length = 10  # Define your prefix length based on your needs\n",
    "model = ClipCaptionModel(prefix_length=prefix_length).to(device)\n",
    "\n",
    "# Define optimizer (e.g., Adam)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-6)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-2)\n",
    "\n",
    "\n",
    "# Define your loss function (e.g., CrossEntropyLoss for text generation)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b8f809fb-687d-42b6-b792-58ecc2b90d99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for epoch in range(1):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "    \n",
    "#     for step, (images, input_ids, attention_masks) in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")):\n",
    "#         images = images.to(device)\n",
    "#         input_ids = input_ids.squeeze(1).to(device)\n",
    "#         attention_mask = attention_masks.squeeze(1).to(device)\n",
    "        \n",
    "\n",
    "#         # Extract features using CLIP\n",
    "#         with torch.no_grad():\n",
    "#             clip_features = clip_model.encode_image(images).float()  # Shape: [batch_size, clip_feature_size]\n",
    "#             print(\"clip features:\",clip_features.size())\n",
    "        \n",
    "#         batch_size=16\n",
    "        \n",
    "        \n",
    "#         # Concatenate the flattened clip features with the input_ids\n",
    "#         input_ids = torch.cat([clip_features, input_ids], dim=1)\n",
    "#         # Ensure input_ids are of type Long (torch.int64)\n",
    "#         input_ids = input_ids.long()\n",
    "#         print(\"input ids size:\",input_ids.size())\n",
    "\n",
    "\n",
    "#         # Check the shape of attention_mask before squeezing\n",
    "#         print(\"Original attention mask shape:\", attention_mask.shape)\n",
    "\n",
    "\n",
    "        \n",
    "#         # Squeeze the middle singleton dimension (axis 1)\n",
    "#         attention_mask_2d = attention_mask.squeeze(1)  # Convert to [batch_size, sequence_length]\n",
    "\n",
    "#         # Check the new shape of the attention mask\n",
    "#         print(\"Attention mask 2d shape:\", attention_mask_2d.shape)\n",
    "        \n",
    "#         # Now concatenate the tensors\n",
    "#         adjusted_attention_mask = torch.cat([torch.ones(batch_size, prefix_length, device=attention_mask_2d.device), attention_mask_2d], dim=1)\n",
    "\n",
    "#         print(\"Adjusted attention mask size:\", adjusted_attention_mask.size())\n",
    "        \n",
    "#         # Ensure the adjusted attention mask has the same length as input_ids\n",
    "#         # assert adjusted_attention_mask.size(1) == input_ids.size(1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "#         print(\"---------------------------------\")\n",
    "#         # # Debugging outputs\n",
    "#         # print(\"Prefix embeddings size:\", clip_features.size())  # Should be [16, prefix_length, embedding_dim]\n",
    "#         # print(\"Input IDs size:\", input_ids.size())  # Should be [16, 50]\n",
    "#         # print(\"Attention masks size:\", attention_masks.size())  # Should be [16, 50]\n",
    "#         # print(\"Adjusted attention mask size:\", adjusted_attention_mask.size())  # Should match total sequence length\n",
    "\n",
    "#         # Obtain GPT-2 embeddings for input tokens\n",
    "#         # gpt2_embeddings = model.gpt2.transformer.wte(input_ids)  # Shape: [batch_size, sequence_length, hidden_size]\n",
    "#         # Access GPT-2 embeddings during training\n",
    "#         gpt2_embeddings = model.gpt.transformer.wte(input_ids)  # Shape: [batch_size, sequence_length, hidden_size]\n",
    "\n",
    "#         print(f\"GPT-2 embeddings size: {gpt2_embeddings.size()}\")\n",
    "        \n",
    "#         # # Concatenate prefix with GPT-2 embeddings\n",
    "#         # inputs_embeds = torch.cat([prefix, gpt2_embeddings], dim=1)  # Concatenate along sequence length\n",
    "#         # print(f\"Concatenated inputs_embeds size: {inputs_embeds.size()}\")\n",
    "\n",
    "\n",
    "#         # Define hidden_size based on the GPT-2 model configuration\n",
    "#         hidden_size = 768\n",
    "#         print(f\"Hidden size: {hidden_size}\")\n",
    "\n",
    "#         print(f\"clip_features shape: {clip_features.size()}\")\n",
    "\n",
    "\n",
    "#         # Get the output from clip_project\n",
    "#         clip_project_output = model.clip_project(clip_features)\n",
    "#         print(f\"clip_project output size: {clip_project_output.size()}\")\n",
    "        \n",
    "#         # Reshape clip_project output\n",
    "#         prefix = clip_project_output.view(batch_size, prefix_length, hidden_size)\n",
    "#         print(f\"Reshaped Prefix size: {prefix.size()}\")\n",
    "        \n",
    "#         # Concatenate prefix with GPT-2 embeddings\n",
    "#         inputs_embeds = torch.cat([prefix, gpt2_embeddings], dim=1)\n",
    "#         print(f\"Concatenated inputs_embeds size: {inputs_embeds.size()}\")\n",
    "\n",
    "        \n",
    "#         # Extend attention mask\n",
    "#         prefix_mask = torch.ones(batch_size, prefix_length).to(attention_mask.device)\n",
    "#         print(f\"Prefix mask shape: {prefix_mask.size()}\")\n",
    "#         print(f\"Attention mask shape: {attention_mask.size()}\")\n",
    "\n",
    "#         extended_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
    "#         print(f\"Extended Attention Mask size: {extended_attention_mask.size()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # Forward pass through the model\n",
    "#         outputs = model(\n",
    "#             tokens=input_ids,\n",
    "#             prefix=clip_features,\n",
    "#             mask= adjusted_attention_mask,  # Use adjusted mask\n",
    "#             labels=input_ids,\n",
    "#         )\n",
    "\n",
    "#         # Compute loss\n",
    "#         loss = outputs.loss / gradient_accumulation_steps\n",
    "\n",
    "#         # Backward pass\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Gradient accumulation\n",
    "#         if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_dataloader):\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#         total_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "#         # Logging\n",
    "#         if (step + 1) % log_interval == 0:\n",
    "#             avg_loss = total_loss / (step + 1)\n",
    "#             print(f\"Step {step + 1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, input_ids, attention_masks in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "#             images = images.to(device)\n",
    "#             input_ids = input_ids.squeeze(1).to(device)\n",
    "#             attention_masks = attention_masks.squeeze(1).to(device)\n",
    "\n",
    "#             clip_features = clip_model.encode_image(images).float()\n",
    "\n",
    "#             # Adjust attention mask for validation\n",
    "#             prefix_length = clip_features.shape[1]\n",
    "#             batch_size, token_length = input_ids.shape\n",
    "#             prefix_mask = torch.ones(batch_size, prefix_length, device=input_ids.device)\n",
    "#             adjusted_attention_mask = torch.cat([prefix_mask, attention_masks], dim=1)\n",
    "\n",
    "#             outputs = model(\n",
    "#                 tokens=input_ids,\n",
    "#                 prefix=clip_features,\n",
    "#                 mask=adjusted_attention_mask,  # Use adjusted mask\n",
    "#                 labels=input_ids,\n",
    "#             )\n",
    "#             val_loss += outputs.loss.item()\n",
    "\n",
    "#     avg_train_loss = total_loss / len(train_dataloader)\n",
    "#     avg_val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1} completed. Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea9d8f66-6137-49a2-8772-d01ca7b36363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "# batch_size=16\n",
    "# hidden_size=768\n",
    "# gradient_accumulation_steps = 1  # Number of steps to accumulate gradients\n",
    "# max_grad_norm = 1.0  # For gradient clipping\n",
    "# log_interval = 10  # Steps after which to log training progress\n",
    "# validation_interval = 50\n",
    "\n",
    "\n",
    "# # Create a subset of the validation dataset with 500 samples\n",
    "# subset_indices = list(range(500))  # Take the first 500 samples\n",
    "# val_subset = Subset(val_dataset, subset_indices)\n",
    "\n",
    "# # Create a dataloader for the subset\n",
    "# val_subset_dataloader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# for epoch in range(1):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for step, (images, input_ids, attention_masks) in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")):\n",
    "#         images = images.to(device)\n",
    "#         input_ids = input_ids.squeeze(1).to(device)\n",
    "#         attention_mask = attention_masks.squeeze(1).to(device)\n",
    "    \n",
    "#         # Extract CLIP features\n",
    "#         with torch.no_grad():\n",
    "#             clip_features = clip_model.encode_image(images).float()  # Shape: [batch_size, clip_feature_size]\n",
    "    \n",
    "#         # Project CLIP features into prefix embeddings\n",
    "#         clip_project_output = model.clip_project(clip_features)  # Shape: [batch_size, gpt_embedding_size * prefix_length]\n",
    "#         prefix = clip_project_output.view(batch_size, prefix_length, hidden_size)  # Shape: [batch_size, prefix_length, hidden_size]\n",
    "    \n",
    "#         # Obtain GPT-2 embeddings for tokens\n",
    "#         gpt2_embeddings = model.gpt.transformer.wte(input_ids)  # Shape: [batch_size, sequence_length, hidden_size]\n",
    "    \n",
    "#         # Concatenate prefix with GPT-2 embeddings\n",
    "#         inputs_embeds = torch.cat([prefix, gpt2_embeddings], dim=1)  # Shape: [batch_size, prefix_length + sequence_length, hidden_size]\n",
    "    \n",
    "#         # Adjust attention mask\n",
    "#         prefix_mask = torch.ones(batch_size, prefix_length, device=attention_mask.device)\n",
    "#         adjusted_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
    "    \n",
    "#         # Forward pass\n",
    "#         outputs = model(\n",
    "#             tokens=input_ids,  # Pass None to use inputs_embeds\n",
    "#             prefix=clip_features,  # Not needed here since we pass inputs_embeds\n",
    "#             mask=adjusted_attention_mask,\n",
    "#             labels=input_ids,\n",
    "#         )\n",
    "    \n",
    "#         # Compute loss\n",
    "#         loss = outputs.loss / gradient_accumulation_steps\n",
    "       \n",
    "    \n",
    "#         # Backward pass\n",
    "#         loss.backward()\n",
    "    \n",
    "#         # Gradient accumulation\n",
    "#         if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_dataloader):\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "    \n",
    "#         total_loss += loss.item() * gradient_accumulation_steps\n",
    "    \n",
    "#         # Logging\n",
    "#         if (step + 1) % log_interval == 0:\n",
    "#             avg_loss = total_loss / (step + 1)\n",
    "#             print(f\"Step {step + 1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "#         # Perform validation after every 'validation_interval' steps\n",
    "#         if (step + 1) % validation_interval == 0:\n",
    "#             model.eval()\n",
    "#             val_loss = 0\n",
    "#             with torch.no_grad():\n",
    "#                 for val_images, val_input_ids, val_attention_masks in tqdm(val_subset_dataloader, desc=\"Validation\", leave = False, disable =True):\n",
    "#                     val_images = val_images.to(device)\n",
    "#                     val_input_ids = val_input_ids.squeeze(1).to(device)\n",
    "#                     val_attention_masks = val_attention_masks.squeeze(1).to(device)\n",
    "\n",
    "#                     val_clip_features = clip_model.encode_image(val_images).float()\n",
    "\n",
    "#                     # Adjust validation attention mask\n",
    "#                     val_prefix_mask = torch.ones(val_images.size(0), prefix_length, device=val_attention_masks.device)\n",
    "#                     val_adjusted_attention_mask = torch.cat([val_prefix_mask, val_attention_masks], dim=1)\n",
    "\n",
    "#                     val_outputs = model(\n",
    "#                         tokens=val_input_ids,\n",
    "#                         prefix=val_clip_features,\n",
    "#                         mask=val_adjusted_attention_mask,\n",
    "#                         labels=val_input_ids,\n",
    "#                     )\n",
    "#                     val_loss += val_outputs.loss.item()\n",
    "\n",
    "#             avg_val_loss = val_loss / len(val_dataloader)\n",
    "#             print(f\"Validation after Step {step + 1}: Avg Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "#             model.train()  # Switch back to training mode\n",
    "\n",
    "#     # Epoch summary\n",
    "#     avg_train_loss = total_loss / len(train_dataloader)\n",
    "#     print(f\"Epoch {epoch + 1} completed. Train Loss: {avg_train_loss:.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bb721daf-27bd-4313-81e8-007118ab1be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check if Adam's internal state (momentum, variance estimates) is restored\n",
    "if optimizer.state_dict()[\"state\"]:\n",
    "    for group in optimizer.state_dict()[\"state\"].values():\n",
    "        print(group.keys())  # Should include \"exp_avg\" and \"exp_avg_sq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c556f-3671-496b-ae38-0e9d84b742c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdsha\\AppData\\Local\\Temp\\ipykernel_24800\\1142457324.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded. Resuming from epoch 6, step 1320.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  53%|                                | 1330/2499 [09:13<14:49,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1330, Loss: 0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  54%|                                | 1340/2499 [09:21<14:51,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1340, Loss: 0.0089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  54%|                               | 1350/2499 [09:28<13:38,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1350, Loss: 0.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  54%|                               | 1360/2499 [09:35<14:35,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1360, Loss: 0.0171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  55%|                               | 1370/2499 [09:43<14:33,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1370, Loss: 0.0215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  55%|                               | 1380/2499 [09:51<14:35,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1380, Loss: 0.0254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  56%|                              | 1390/2499 [09:59<14:38,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1390, Loss: 0.0290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  56%|                              | 1399/2499 [10:05<13:58,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1400, Loss: 0.0327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  56%|                            | 1400/2499 [21:59<65:30:16, 214.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Step 1400: Avg Val Loss: 0.6762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  56%|                             | 1410/2499 [22:07<2:03:52,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1410, Loss: 0.0363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  57%|                             | 1420/2499 [22:15<16:58,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1420, Loss: 0.0402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  57%|                             | 1430/2499 [22:22<13:30,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1430, Loss: 0.0441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  58%|                             | 1440/2499 [22:30<13:37,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1440, Loss: 0.0477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  58%|                             | 1450/2499 [22:37<13:04,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1450, Loss: 0.0511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  58%|                            | 1460/2499 [22:45<12:51,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1460, Loss: 0.0548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  59%|                            | 1470/2499 [22:52<12:43,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1470, Loss: 0.0582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  59%|                            | 1480/2499 [23:00<13:13,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1480, Loss: 0.0618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  60%|                           | 1490/2499 [23:08<12:49,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1490, Loss: 0.0651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  60%|                           | 1499/2499 [23:15<13:06,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1500, Loss: 0.0684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  60%|                          | 1500/2499 [23:33<1:40:33,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Step 1500: Avg Val Loss: 0.6785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  60%|                           | 1510/2499 [23:41<14:50,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1510, Loss: 0.0715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  61%|                           | 1520/2499 [23:48<12:10,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1520, Loss: 0.0744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  61%|                          | 1530/2499 [23:56<12:15,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1530, Loss: 0.0772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  62%|                          | 1540/2499 [24:03<12:29,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1540, Loss: 0.0802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  62%|                          | 1550/2499 [24:11<12:05,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1550, Loss: 0.0833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  62%|                          | 1560/2499 [24:18<12:04,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1560, Loss: 0.0861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  63%|                         | 1570/2499 [24:26<11:13,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1570, Loss: 0.0894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  63%|                         | 1580/2499 [24:33<11:20,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1580, Loss: 0.0923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  64%|                         | 1590/2499 [24:41<11:06,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1590, Loss: 0.0952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  64%|                        | 1599/2499 [24:47<10:47,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1600, Loss: 0.0980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  64%|                        | 1600/2499 [25:05<1:28:34,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Step 1600: Avg Val Loss: 0.6770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  64%|                        | 1610/2499 [25:13<12:44,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1610, Loss: 0.1010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  65%|                        | 1620/2499 [25:20<11:48,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1620, Loss: 0.1037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  65%|                        | 1630/2499 [25:28<10:58,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1630, Loss: 0.1065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  66%|                       | 1640/2499 [25:35<10:15,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1640, Loss: 0.1090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  66%|                       | 1650/2499 [25:43<11:01,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1650, Loss: 0.1115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  66%|                       | 1660/2499 [25:51<10:45,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1660, Loss: 0.1142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  67%|                       | 1670/2499 [25:58<10:14,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1670, Loss: 0.1170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  67%|                      | 1680/2499 [26:06<10:01,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1680, Loss: 0.1199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  68%|                      | 1690/2499 [26:14<10:37,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1690, Loss: 0.1226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  68%|                      | 1699/2499 [26:20<09:32,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1700, Loss: 0.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  68%|                     | 1700/2499 [26:38<1:19:41,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Step 1700: Avg Val Loss: 0.6759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  68%|                     | 1710/2499 [26:46<11:34,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1710, Loss: 0.1273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  69%|                     | 1720/2499 [26:54<10:08,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1720, Loss: 0.1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  69%|                     | 1730/2499 [27:01<09:42,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1730, Loss: 0.1327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  70%|                     | 1740/2499 [27:09<09:30,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1740, Loss: 0.1353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  70%|                    | 1750/2499 [27:16<09:14,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1750, Loss: 0.1376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  70%|                    | 1760/2499 [27:24<09:42,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1760, Loss: 0.1397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  71%|                    | 1770/2499 [27:31<08:55,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1770, Loss: 0.1421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  71%|                   | 1780/2499 [27:39<09:01,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1780, Loss: 0.1445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  72%|                   | 1790/2499 [27:46<09:03,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1790, Loss: 0.1465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  72%|                   | 1799/2499 [27:53<08:33,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1800, Loss: 0.1490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  72%|                  | 1800/2499 [28:12<1:10:21,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Step 1800: Avg Val Loss: 0.6777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  72%|                   | 1810/2499 [28:19<10:21,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1810, Loss: 0.1513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  73%|                  | 1820/2499 [28:27<08:49,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1820, Loss: 0.1537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  73%|                  | 1830/2499 [28:35<08:38,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1830, Loss: 0.1561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  74%|                  | 1840/2499 [28:42<08:08,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1840, Loss: 0.1583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  74%|                  | 1850/2499 [28:50<07:54,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1850, Loss: 0.1605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  74%|                 | 1860/2499 [28:57<07:57,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1860, Loss: 0.1630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  75%|                 | 1870/2499 [29:05<07:57,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1870, Loss: 0.1651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  75%|                 | 1880/2499 [29:12<07:50,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1880, Loss: 0.1672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  76%|                | 1890/2499 [29:20<07:53,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1890, Loss: 0.1694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  76%|                | 1899/2499 [29:27<07:48,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1900, Loss: 0.1714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  76%|                | 1900/2499 [29:46<1:01:03,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Step 1900: Avg Val Loss: 0.6768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  76%|                | 1910/2499 [29:53<09:04,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1910, Loss: 0.1730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  77%|                | 1920/2499 [30:01<07:54,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1920, Loss: 0.1753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  77%|               | 1930/2499 [30:09<07:29,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1930, Loss: 0.1770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  78%|               | 1940/2499 [30:17<07:31,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1940, Loss: 0.1791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  78%|               | 1950/2499 [30:24<06:59,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1950, Loss: 0.1810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  78%|               | 1960/2499 [30:32<06:47,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1960, Loss: 0.1828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  79%|              | 1970/2499 [30:40<06:46,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1970, Loss: 0.1845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  79%|              | 1980/2499 [30:47<06:33,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1980, Loss: 0.1866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  80%|              | 1990/2499 [30:55<06:33,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1990, Loss: 0.1882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  80%|             | 1999/2499 [31:02<06:46,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000, Loss: 0.1902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  80%|             | 2000/2499 [31:20<50:36,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Step 2000: Avg Val Loss: 0.6783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  80%|             | 2010/2499 [31:28<07:34,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2010, Loss: 0.1919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  81%|             | 2020/2499 [31:36<06:07,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2020, Loss: 0.1937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  81%|             | 2030/2499 [31:43<05:54,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2030, Loss: 0.1956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  82%|            | 2040/2499 [31:51<05:53,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2040, Loss: 0.1976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  82%|            | 2050/2499 [31:59<05:41,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2050, Loss: 0.1992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  82%|            | 2060/2499 [32:06<05:43,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2060, Loss: 0.2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  83%|           | 2070/2499 [32:14<05:22,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2070, Loss: 0.2029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  83%|           | 2080/2499 [32:21<05:13,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2080, Loss: 0.2045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  84%|           | 2090/2499 [32:29<05:04,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2090, Loss: 0.2061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  84%|           | 2099/2499 [32:36<05:06,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2100, Loss: 0.2078\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "batch_size=16\n",
    "hidden_size=768\n",
    "gradient_accumulation_steps = 1  # Number of steps to accumulate gradients\n",
    "max_grad_norm = 1.0  # For gradient clipping\n",
    "log_interval = 10  # Steps after which to log training progress\n",
    "validation_interval = 100\n",
    "\n",
    "# Create a subset of the validation dataset with 500 samples\n",
    "subset_indices = list(range(500))  # Take the first 500 samples\n",
    "val_subset = Subset(val_dataset, subset_indices)\n",
    "\n",
    "# Create a dataloader for the subset\n",
    "val_subset_dataloader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Checkpoint path\n",
    "checkpoint_path = r\"C:\\Users\\vdsha\\Downloads\\model_checkpoint_epoch4.pth\"\n",
    "\n",
    "# Function to save checkpoint\n",
    "def save_checkpoint(model, optimizer, epoch, step, total_loss, checkpoint_path):\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"step\": step,\n",
    "        \"total_loss\": total_loss,\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch}, step {step}.\")\n",
    "\n",
    "# Function to load checkpoint\n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        epoch = checkpoint[\"epoch\"]\n",
    "        step = checkpoint[\"step\"]\n",
    "        total_loss = checkpoint[\"total_loss\"]\n",
    "        print(f\"Checkpoint loaded. Resuming from epoch {epoch}, step {step}.\")\n",
    "        return epoch, step, total_loss\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting from scratch.\")\n",
    "        return 0, 0, 0.0\n",
    "\n",
    "# Load checkpoint if exists\n",
    "start_epoch, start_step, total_loss = load_checkpoint(checkpoint_path, model, optimizer)\n",
    "# # Debug: Check if Adam's internal state (momentum, variance estimates) is restored\n",
    "# if optimizer.state_dict()[\"state\"]:\n",
    "#     for group in optimizer.state_dict()[\"state\"].values():\n",
    "#         print(group.keys())  # Should include \"exp_avg\" and \"exp_avg_sq\"\n",
    "\n",
    "\n",
    "try:\n",
    "    for epoch in range(start_epoch, 8):  # Adjust range as needed\n",
    "        model.train()\n",
    "        total_loss = 0.0  # Reset total loss for the new epoch\n",
    "        if epoch == start_epoch:\n",
    "            start = start_step\n",
    "        else:\n",
    "            start = 0\n",
    "\n",
    "        for step, (images, input_ids, attention_masks) in enumerate(\n",
    "            tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")\n",
    "        ):\n",
    "            if step < start:\n",
    "                continue  # Skip steps already completed\n",
    "\n",
    "            images = images.to(device)\n",
    "            input_ids = input_ids.squeeze(1).to(device)\n",
    "            attention_mask = attention_masks.squeeze(1).to(device)\n",
    "\n",
    "            # Extract CLIP features\n",
    "            with torch.no_grad():\n",
    "                clip_features = clip_model.encode_image(images).float()\n",
    "\n",
    "            # Project CLIP features into prefix embeddings\n",
    "            clip_project_output = model.clip_project(clip_features)\n",
    "            prefix = clip_project_output.view(batch_size, prefix_length, hidden_size)\n",
    "\n",
    "            # Obtain GPT-2 embeddings for tokens\n",
    "            gpt2_embeddings = model.gpt.transformer.wte(input_ids)\n",
    "\n",
    "            # Concatenate prefix with GPT-2 embeddings\n",
    "            inputs_embeds = torch.cat([prefix, gpt2_embeddings], dim=1)\n",
    "\n",
    "            # Adjust attention mask\n",
    "            prefix_mask = torch.ones(batch_size, prefix_length, device=attention_mask.device)\n",
    "            adjusted_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                tokens=input_ids,\n",
    "                prefix=clip_features,\n",
    "                mask=adjusted_attention_mask,\n",
    "                labels=input_ids,\n",
    "            )\n",
    "\n",
    "            # Compute loss\n",
    "            loss = outputs.loss / gradient_accumulation_steps\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # # Debugging: Check gradient norms\n",
    "            # for name, param in model.named_parameters():\n",
    "            #     if param.grad is not None:\n",
    "            #         print(f\"{name}: grad norm = {torch.norm(param.grad)}\")\n",
    "\n",
    "            # Gradient accumulation\n",
    "            if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_dataloader):\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "            # Logging\n",
    "            if (step + 1) % log_interval == 0:\n",
    "                avg_loss = total_loss / (step + 1)\n",
    "                print(f\"Step {step + 1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Perform validation\n",
    "            if (step + 1) % validation_interval == 0:\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                with torch.no_grad():\n",
    "                    for val_images, val_input_ids, val_attention_masks in tqdm(val_subset_dataloader, desc=\"Validation\", leave = False, disable =True):\n",
    "                        val_images = val_images.to(device)\n",
    "                        val_input_ids = val_input_ids.squeeze(1).to(device)\n",
    "                        val_attention_masks = val_attention_masks.squeeze(1).to(device)\n",
    "    \n",
    "                        val_clip_features = clip_model.encode_image(val_images).float()\n",
    "    \n",
    "                        # Adjust validation attention mask\n",
    "                        val_prefix_mask = torch.ones(val_images.size(0), prefix_length, device=val_attention_masks.device)\n",
    "                        val_adjusted_attention_mask = torch.cat([val_prefix_mask, val_attention_masks], dim=1)\n",
    "    \n",
    "                        val_outputs = model(\n",
    "                            tokens=val_input_ids,\n",
    "                            prefix=val_clip_features,\n",
    "                            mask=val_adjusted_attention_mask,\n",
    "                            labels=val_input_ids,\n",
    "                        )\n",
    "                        val_loss += val_outputs.loss.item()\n",
    "    \n",
    "                avg_val_loss = val_loss / len(val_subset_dataloader)\n",
    "                print(f\"Validation after Step {step + 1}: Avg Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "                model.train()  # Switch back to training mode\n",
    "\n",
    "\n",
    "        # Epoch summary\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1} completed. Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Saving checkpoint...\")\n",
    "    save_checkpoint(model, optimizer, epoch, step, total_loss, checkpoint_path)\n",
    "\n",
    "# save_checkpoint(model, optimizer, epoch, step, total_loss, checkpoint_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e8c91fb-1461-4d0f-89dd-9bb028557603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"prefix size:\", prefix.size())\n",
    "# print(\"input_ids size:\", input_ids.size())\n",
    "# print(\"attention_masks size:\", attention_masks.size())\n",
    "# print(\"extended_attention_masks size:\", extended_attention_masks.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d91d925b-71b8-4f85-982a-f6169923c2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at epoch 4, step 2498.\n"
     ]
    }
   ],
   "source": [
    "save_checkpoint(model, optimizer, epoch, step, total_loss, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2e2e0f2-fb70-4590-9a9d-111288fcfc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# checkpoint = torch.load(r\"C:\\Users\\vdsha\\Downloads\\model_checkpoint_epoch1.pth\")\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "# # Set the model to evaluation mode\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32408b5e-5d75-4585-9140-4041620a2930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")  # Replace \"gpt2\" with your tokenizer if customized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d0c2b3b-4fa7-45db-a352-8b5e972a86ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdsha\\AppData\\Local\\Temp\\ipykernel_7992\\4257109114.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
      "C:\\Users\\vdsha\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:540: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: A blue bottle of shampoo is on a table.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import GPT2Tokenizer\n",
    "from torchvision import transforms\n",
    "\n",
    "# Paths\n",
    "image_path = r\"D:\\vidisha\\vizwiz\\real time test imgs\\nivea.jpg\"\n",
    "checkpoint_path = r\"C:\\Users\\vdsha\\Downloads\\model_checkpoint_epoch4_new.pth\"\n",
    "# Load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ClipCaptionModel(prefix_length=10).to(device)  # Adjust prefix_length as per your training\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)  # Use the same optimizer as training\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# Load the CLIP model\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Preprocess the image\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "preprocessed_image = clip_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "\n",
    "# Extract CLIP features\n",
    "with torch.no_grad():\n",
    "    clip_features = clip_model.get_image_features(preprocessed_image).float()\n",
    "\n",
    "\n",
    "\n",
    "# Load GPT-2 tokenizer and add special tokens\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Add BOS, EOS, and PAD tokens\n",
    "special_tokens = {'bos_token': '<start>', 'eos_token': '<end>', 'pad_token': '<pad>'}\n",
    "gpt2_tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Update pad token ID\n",
    "gpt2_tokenizer.pad_token_id = gpt2_tokenizer.convert_tokens_to_ids(\"<pad>\")\n",
    "\n",
    "# Ensure the model knows about the new tokens\n",
    "model.gpt.resize_token_embeddings(len(gpt2_tokenizer))\n",
    "\n",
    "# Use special tokens\n",
    "start_token_id = gpt2_tokenizer.bos_token_id\n",
    "end_token_id = gpt2_tokenizer.eos_token_id\n",
    "\n",
    "max_length = 100  # Adjust this as needed\n",
    "\n",
    "# Generate caption\n",
    "# with torch.no_grad():\n",
    "#     tokens = torch.tensor([[start_token_id]], device=device)\n",
    "#     prefix = model.clip_project(clip_features).view(1, model.prefix_length, model.gpt_embedding_size)\n",
    "\n",
    "#     for _ in range(max_length):\n",
    "#         outputs = model.gpt(inputs_embeds=torch.cat((prefix, model.gpt.transformer.wte(tokens)), dim=1))\n",
    "#         logits = outputs.logits\n",
    "#         next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "#         tokens = torch.cat((tokens, next_token.unsqueeze(0)), dim=1)\n",
    "\n",
    "#         if next_token.item() == end_token_id:\n",
    "#             break\n",
    "\n",
    "# caption = gpt2_tokenizer.decode(tokens.squeeze().tolist(), skip_special_tokens=True)\n",
    "# print(f\"Generated Caption: {caption}\")\n",
    "\n",
    "# Generate caption\n",
    "with torch.no_grad():\n",
    "    tokens = torch.tensor([[start_token_id]], device=device)\n",
    "    prefix = model.clip_project(clip_features).view(1, model.prefix_length, model.gpt_embedding_size)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        outputs = model.gpt(inputs_embeds=torch.cat((prefix, model.gpt.transformer.wte(tokens)), dim=1))\n",
    "        logits = outputs.logits\n",
    "        next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "        # Stop if the <end> token is generated\n",
    "        if next_token.item() == end_token_id:\n",
    "            break\n",
    "\n",
    "        tokens = torch.cat((tokens, next_token.unsqueeze(0)), dim=1)\n",
    "\n",
    "# Decode and remove unnecessary repetitions\n",
    "caption = gpt2_tokenizer.decode(tokens.squeeze().tolist(), skip_special_tokens=True)\n",
    "caption = caption.split(\"<end>\")[0].strip()  # Keep only the first valid caption\n",
    "caption = caption.replace(\"<start>\", \"\").strip()\n",
    "\n",
    "# Debugging output\n",
    "# print(\"Generated Token IDs:\", tokens.tolist())\n",
    "print(f\"Generated Caption: {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24608083-3994-4ac8-ade8-484e66c4ecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdsha\\AppData\\Local\\Temp\\ipykernel_18524\\3441623897.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
      "C:\\Users\\vdsha\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\easyocr\\detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
      "C:\\Users\\vdsha\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\easyocr\\recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Caption: SLICED PEACHEY A can of minced garlic sitting on a countertop.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SLICED PEACHEY A can of minced garlic sitting on a countertop.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "# from PIL import Image\n",
    "# import easyocr\n",
    "# from transformers import GPT2Tokenizer, CLIPProcessor, CLIPModel\n",
    "\n",
    "# def load_models():\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "#     # Load fine-tuned CLIP-GPT2 model\n",
    "#     model = ClipCaptionModel(prefix_length=10).to(device)\n",
    "#     checkpoint_path = r\"C:\\Users\\vdsha\\Downloads\\model_checkpoint_epoch4_new.pth\"  # Update with actual path\n",
    "#     checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "#     model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "#     model.eval()\n",
    "    \n",
    "#     # Load CLIP model\n",
    "#     clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "#     clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "#     # Load tokenizer\n",
    "#     tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "#     tokenizer.add_special_tokens({'bos_token': '<start>', 'eos_token': '<end>', 'pad_token': '<pad>'})\n",
    "#     tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(\"<pad>\")\n",
    "#     model.gpt.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "#     return model, clip_model, clip_processor, tokenizer, device\n",
    "\n",
    "# def get_ocr_text(image_path):\n",
    "#     reader = easyocr.Reader([\"en\"])  \n",
    "#     results = reader.readtext(image_path)\n",
    "#     extracted_text = \" \".join([res[1] for res in results])  \n",
    "\n",
    "#     # Clean up detected text (remove numbers, extra words, etc.)\n",
    "#     words = extracted_text.split()\n",
    "#     filtered_words = [word for word in words if len(word) > 2 and not word.isnumeric()]\n",
    "#     extracted_text = \" \".join(filtered_words[:4])  # Limit to 4 words\n",
    "    \n",
    "#     return extracted_text.strip()\n",
    "\n",
    "# def generate_caption(image_path, model, clip_model, clip_processor, tokenizer, ocr_text, device):\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     preprocessed_image = clip_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         clip_features = clip_model.get_image_features(preprocessed_image).float()\n",
    "    \n",
    "#     start_token_id = tokenizer.bos_token_id\n",
    "#     end_token_id = tokenizer.eos_token_id\n",
    "#     max_length = 50  \n",
    "\n",
    "#     # Convert OCR text into tokens and filter duplication\n",
    "#     ocr_tokens = tokenizer.encode(ocr_text, add_special_tokens=False) if ocr_text else []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         prefix = model.clip_project(clip_features).view(1, model.prefix_length, model.gpt_embedding_size)\n",
    "#         tokens = torch.tensor([[start_token_id]], device=device)\n",
    "\n",
    "#         for _ in range(max_length):\n",
    "#             outputs = model.gpt(inputs_embeds=torch.cat((prefix, model.gpt.transformer.wte(tokens)), dim=1))\n",
    "#             logits = outputs.logits[:, -1, :]\n",
    "#             next_token = torch.argmax(logits, dim=-1)\n",
    "\n",
    "#             if next_token.item() == end_token_id:\n",
    "#                 break\n",
    "#             tokens = torch.cat((tokens, next_token.unsqueeze(0)), dim=1)\n",
    "    \n",
    "#     caption = tokenizer.decode(tokens.squeeze().tolist(), skip_special_tokens=True)\n",
    "#     caption = caption.replace(\"<start>\", \"\").split(\"<end>\")[0].strip()\n",
    "\n",
    "#     # **Fix duplication: Remove repeated OCR words in caption**\n",
    "#     if ocr_text:\n",
    "#         ocr_words = set(ocr_text.lower().split())\n",
    "#         caption_words = caption.split()\n",
    "#         caption = \" \".join([word for word in caption_words if word.lower() not in ocr_words])\n",
    "\n",
    "#         # Reinsert OCR text in a natural way\n",
    "#         caption = f\"{ocr_text} {caption}\".strip()\n",
    "\n",
    "#     return caption\n",
    "\n",
    "# def main(image_path):\n",
    "#     model, clip_model, clip_processor, tokenizer, device = load_models()\n",
    "    \n",
    "#     # Get detected text from EasyOCR\n",
    "#     ocr_text = get_ocr_text(image_path)\n",
    "    \n",
    "#     # Generate caption with **seamless OCR integration**\n",
    "#     final_caption = generate_caption(image_path, model, clip_model, clip_processor, tokenizer, ocr_text, device)\n",
    "\n",
    "#     print(\"Final Caption:\", final_caption)\n",
    "#     return final_caption\n",
    "\n",
    "# # Example usage\n",
    "# image_path = r\"D:\\vidisha\\vizwiz\\test\\VizWiz_test_00000046.jpg\"  # Update with actual path\n",
    "# main(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b4ebd5f-8024-4945-a326-a8cb3eafc946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "C:\\Users\\vdsha\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\easyocr\\detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
      "C:\\Users\\vdsha\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\easyocr\\recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
      "C:\\Users\\vdsha\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:540: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NIVEA Body Mllk Shea Smoothie  A bottle of body lotion is on a table.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import PIL\n",
    "if not hasattr(PIL.Image, \"ANTIALIAS\"):  \n",
    "    PIL.Image.ANTIALIAS = PIL.Image.LANCZOS  # Redirect ANTIALIAS to LANCZOS\n",
    "\n",
    "import easyocr\n",
    "from transformers import GPT2Tokenizer, CLIPProcessor, CLIPModel\n",
    "\n",
    "def load_models():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load fine-tuned CLIP-GPT2 model\n",
    "    model = ClipCaptionModel(prefix_length=10).to(device)\n",
    "    checkpoint_path = r\"C:\\Users\\vdsha\\Downloads\\model_checkpoint_epoch4_new.pth\" # Update path\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only= True)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    \n",
    "    # Load CLIP model\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.add_special_tokens({'bos_token': '<start>', 'eos_token': '<end>', 'pad_token': '<pad>'})\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(\"<pad>\")\n",
    "    model.gpt.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    return model, clip_model, clip_processor, tokenizer, device\n",
    "\n",
    "def get_ocr_text(image_path):\n",
    "    reader = easyocr.Reader([\"en\"])  # Load EasyOCR for English\n",
    "    results = reader.readtext(image_path)\n",
    "    extracted_text = \" \".join([res[1] for res in results])  # Combine detected words\n",
    "\n",
    "    # Clean up detected text (remove single characters, numbers, etc.)\n",
    "    words = extracted_text.split()\n",
    "    filtered_words = [word for word in words if len(word) > 2 and not word.isnumeric()]\n",
    "    extracted_text = \" \".join(filtered_words[:5])  # Limit to 5 words for better integration\n",
    "    \n",
    "    return extracted_text.strip()\n",
    "\n",
    "def generate_caption(image_path, model, clip_model, clip_processor, tokenizer, ocr_text, device):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocessed_image = clip_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        clip_features = clip_model.get_image_features(preprocessed_image).float()\n",
    "    \n",
    "    start_token_id = tokenizer.bos_token_id\n",
    "    end_token_id = tokenizer.eos_token_id\n",
    "    max_length = 50  \n",
    "\n",
    "    # **Inject OCR text into token sequence**\n",
    "    tokens = [start_token_id]  # Start sequence\n",
    "    \n",
    "    if ocr_text:  \n",
    "        ocr_tokens = tokenizer.encode(ocr_text, add_special_tokens=False)  # Convert OCR text to tokens\n",
    "        tokens.extend(ocr_tokens)  # Merge OCR tokens **before** caption generation\n",
    "\n",
    "    tokens = torch.tensor([tokens], device=device)  # Convert to tensor\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prefix = model.clip_project(clip_features).view(1, model.prefix_length, model.gpt_embedding_size)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            outputs = model.gpt(inputs_embeds=torch.cat((prefix, model.gpt.transformer.wte(tokens)), dim=1))\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            if next_token.item() == end_token_id:\n",
    "                break\n",
    "            tokens = torch.cat((tokens, next_token.unsqueeze(0)), dim=1)\n",
    "    \n",
    "    caption = tokenizer.decode(tokens.squeeze().tolist(), skip_special_tokens=True)\n",
    "    caption = caption.replace(\"<start>\", \"\").split(\"<end>\")[0].strip()\n",
    "\n",
    "    return caption\n",
    "\n",
    "def main(image_path):\n",
    "    model, clip_model, clip_processor, tokenizer, device = load_models()\n",
    "    \n",
    "    # Get detected text from EasyOCR\n",
    "    ocr_text = get_ocr_text(image_path)\n",
    "    \n",
    "    # Generate caption with **embedded OCR text**\n",
    "    final_caption = generate_caption(image_path, model, clip_model, clip_processor, tokenizer, ocr_text, device)\n",
    "\n",
    "    # print(\"Final Caption:\", final_caption)\n",
    "    return final_caption\n",
    "\n",
    "# Example usage: \"D:\\vidisha\\vizwiz\\test\\VizWiz_test_00000474.jpg\",\"D:\\vidisha\\vizwiz\\real time test imgs\\nivea.jpg\", \"D:\\vidisha\\vizwiz\\test\\VizWiz_test_00000046.jpg\", \"D:\\vidisha\\vizwiz\\test\\VizWiz_test_00000148.jpg\", \"D:\\vidisha\\vizwiz\\test\\VizWiz_test_00000181.jpg\"\n",
    "image_path = r\"D:\\vidisha\\vizwiz\\real time test imgs\\nivea.jpg\"# Update with actual path\n",
    "main(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "43687390-1429-4201-bea8-72daf6587b34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdsha\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\easyocr\\detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
      "C:\\Users\\vdsha\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\easyocr\\recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Caption: NIVEA Body Mllk Shea Smoothie  A bottle of body lotion is on a table.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NIVEA Body Mllk Shea Smoothie  A bottle of body lotion is on a table.'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import easyocr\n",
    "from transformers import GPT2Tokenizer, CLIPProcessor, CLIPModel\n",
    "import PIL\n",
    "if not hasattr(PIL.Image, \"ANTIALIAS\"):  \n",
    "    PIL.Image.ANTIALIAS = PIL.Image.LANCZOS  # Redirect ANTIALIAS to LANCZOS\n",
    "\n",
    "\n",
    "def load_models():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load fine-tuned CLIP-GPT2 model\n",
    "    model = ClipCaptionModel(prefix_length=10).to(device)\n",
    "    checkpoint_path = r\"C:\\Users\\vdsha\\Downloads\\model_checkpoint_epoch4_new.pth\" # Update path\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only= True)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    \n",
    "    # Load CLIP model\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.add_special_tokens({'bos_token': '<start>', 'eos_token': '<end>', 'pad_token': '<pad>'})\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(\"<pad>\")\n",
    "    model.gpt.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    return model, clip_model, clip_processor, tokenizer, device\n",
    "\n",
    "def get_ocr_text(image_path):\n",
    "    reader = easyocr.Reader([\"en\"])  # Load EasyOCR for English\n",
    "    results = reader.readtext(image_path)\n",
    "    extracted_text = \" \".join([res[1] for res in results])  # Combine detected words\n",
    "\n",
    "    # Clean up detected text (remove single characters, numbers, etc.)\n",
    "    words = extracted_text.split()\n",
    "    filtered_words = [word for word in words if len(word) > 2 and not word.isnumeric()]\n",
    "    extracted_text = \" \".join(filtered_words[:5])  # Limit to 5 words for better integration\n",
    "    \n",
    "    return extracted_text.strip()\n",
    "\n",
    "# def generate_caption(image_path, model, clip_model, clip_processor, tokenizer, ocr_text, device):\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     preprocessed_image = clip_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         clip_features = clip_model.get_image_features(preprocessed_image).float()\n",
    "    \n",
    "#     start_token_id = tokenizer.bos_token_id\n",
    "#     end_token_id = tokenizer.eos_token_id\n",
    "#     max_length = 50  \n",
    "\n",
    "#     # **Inject OCR text into token sequence**\n",
    "#     tokens = [start_token_id]  # Start sequence\n",
    "    \n",
    "#     if ocr_text:  \n",
    "#         ocr_tokens = tokenizer.encode(ocr_text, add_special_tokens=False)  # Convert OCR text to tokens\n",
    "#         tokens.extend(ocr_tokens)  # Merge OCR tokens **before** caption generation\n",
    "\n",
    "#     tokens = torch.tensor([tokens], device=device)  # Convert to tensor\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         prefix = model.clip_project(clip_features).view(1, model.prefix_length, model.gpt_embedding_size)\n",
    "        \n",
    "#         for _ in range(max_length):\n",
    "#             outputs = model.gpt(inputs_embeds=torch.cat((prefix, model.gpt.transformer.wte(tokens)), dim=1))\n",
    "#             logits = outputs.logits[:, -1, :]\n",
    "#             next_token = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "#             if next_token.item() == end_token_id:\n",
    "#                 break\n",
    "#             tokens = torch.cat((tokens, next_token.unsqueeze(0)), dim=1)\n",
    "    \n",
    "#     caption = tokenizer.decode(tokens.squeeze().tolist(), skip_special_tokens=True)\n",
    "#     caption = caption.replace(\"<start>\", \"\").split(\"<end>\")[0].strip()\n",
    "\n",
    "#     return caption\n",
    "\n",
    "# def generate_caption(image_path, model, clip_model, clip_processor, tokenizer, ocr_text, device):\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     preprocessed_image = clip_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         clip_features = clip_model.get_image_features(preprocessed_image).float()\n",
    "\n",
    "#     start_token_id = tokenizer.bos_token_id\n",
    "#     end_token_id = tokenizer.eos_token_id\n",
    "#     max_length = 50  \n",
    "\n",
    "#     tokens = torch.tensor([[start_token_id]], device=device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         prefix = model.clip_project(clip_features).view(1, model.prefix_length, model.gpt_embedding_size)\n",
    "\n",
    "#         for i in range(max_length):\n",
    "#             outputs = model.gpt(inputs_embeds=torch.cat((prefix, model.gpt.transformer.wte(tokens)), dim=1))\n",
    "#             logits = outputs.logits[:, -1, :]\n",
    "#             next_token = torch.argmax(logits, dim=-1)\n",
    "\n",
    "#             # Insert OCR text **after the first few tokens** for natural integration\n",
    "#             if i == 2 and ocr_text:  \n",
    "#                 ocr_tokens = tokenizer.encode(ocr_text, add_special_tokens=False)  # Convert OCR text to tokens\n",
    "#                 ocr_tokens = torch.tensor([ocr_tokens], device=device)\n",
    "#                 tokens = torch.cat((tokens, ocr_tokens), dim=1)\n",
    "\n",
    "#             if next_token.item() == end_token_id:\n",
    "#                 break\n",
    "\n",
    "#             tokens = torch.cat((tokens, next_token.unsqueeze(0)), dim=1)\n",
    "\n",
    "    # caption = tokenizer.decode(tokens.squeeze().tolist(), skip_special_tokens=True)\n",
    "    # caption = caption.replace(\"<start>\", \"\").split(\"<end>\")[0].strip()\n",
    "\n",
    "    # return caption\n",
    "\n",
    "def generate_caption(image_path, model, clip_model, clip_processor, tokenizer, ocr_text, device):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocessed_image = clip_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        clip_features = clip_model.get_image_features(preprocessed_image).float()\n",
    "\n",
    "    start_token_id = tokenizer.bos_token_id\n",
    "    end_token_id = tokenizer.eos_token_id\n",
    "    max_length = 50  \n",
    "\n",
    "    tokens = [start_token_id]  # Start sequence\n",
    "\n",
    "    if ocr_text:  \n",
    "        #  If OCR text is available, let GPT-2 use it as the **main subject**\n",
    "        ocr_tokens = tokenizer.encode(ocr_text, add_special_tokens=False)\n",
    "        tokens.extend(ocr_tokens)\n",
    "    else:\n",
    "        #  No OCR? Let GPT-2 generate normally from the start\n",
    "        tokens = [start_token_id]\n",
    "\n",
    "    tokens = torch.tensor([tokens], device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prefix = model.clip_project(clip_features).view(1, model.prefix_length, model.gpt_embedding_size)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            outputs = model.gpt(inputs_embeds=torch.cat((prefix, model.gpt.transformer.wte(tokens)), dim=1))\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            if next_token.item() == end_token_id:\n",
    "                break\n",
    "            tokens = torch.cat((tokens, next_token.unsqueeze(0)), dim=1)\n",
    "\n",
    "    caption = tokenizer.decode(tokens.squeeze().tolist(), skip_special_tokens=True)\n",
    "    caption = caption.replace(\"<start>\", \"\").split(\"<end>\")[0].strip()\n",
    "\n",
    "    return caption\n",
    "\n",
    "def main(image_path):\n",
    "    model, clip_model, clip_processor, tokenizer, device = load_models()\n",
    "    \n",
    "    # Get detected text from EasyOCR\n",
    "    ocr_text = get_ocr_text(image_path)\n",
    "    \n",
    "    # Generate caption with **embedded OCR text**\n",
    "    final_caption = generate_caption(image_path, model, clip_model, clip_processor, tokenizer, ocr_text, device)\n",
    "\n",
    "    print(\"Final Caption:\", final_caption)\n",
    "    return final_caption\n",
    "\n",
    "# Example usage\n",
    "image_path = r\"D:\\vidisha\\vizwiz\\real time test imgs\\nivea.jpg\"# Update with actual path\n",
    "main(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "47fd2f32-aa07-472c-97dd-13de1e4deb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdsha\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\easyocr\\detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
      "C:\\Users\\vdsha\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\easyocr\\recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OCR]: Smile\n",
      "Final Caption: Smile.  A woman is wearing a pink t-shirt with a woman in the background.  A woman is wearing a blue t-shirt with a woman in the background.\n"
     ]
    }
   ],
   "source": [
    "# def generate_caption(image_path, model, clip_model, gpt2_tokenizer, device, max_length=50):\n",
    "#     \"\"\"Generate image caption using CLIP-GPT2 with OCR conditioning.\"\"\"\n",
    "\n",
    "#     # Load and preprocess the image\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     preprocessed_image = clip_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "\n",
    "#     # Extract CLIP features\n",
    "#     with torch.no_grad():\n",
    "#         clip_features = clip_model.get_image_features(preprocessed_image).float()\n",
    "\n",
    "#     # Extract OCR text\n",
    "#     ocr_text = get_ocr_text(image_path)\n",
    "#     print(f\"[OCR]: {ocr_text}\")\n",
    "\n",
    "#     # Construct the prompt for GPT-2\n",
    "#     if ocr_text.strip():\n",
    "#         prompt = f\"Detected text: {ocr_text}. Caption:\"\n",
    "#     else:\n",
    "#         prompt = \"Caption:\"\n",
    "\n",
    "#     # Tokenize the prompt\n",
    "#     input_ids = gpt2_tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "#     # Project CLIP features\n",
    "#     prefix = model.clip_project(clip_features).view(1, model.prefix_length, model.gpt_embedding_size)\n",
    "\n",
    "#     # Generate caption\n",
    "#     with torch.no_grad():\n",
    "#         for _ in range(max_length):\n",
    "#             # Embed input tokens\n",
    "#             gpt2_embeddings = model.gpt.transformer.wte(input_ids)\n",
    "#             inputs_embeds = torch.cat((prefix, gpt2_embeddings), dim=1)\n",
    "\n",
    "#             # Generate logits\n",
    "#             outputs = model.gpt(inputs_embeds=inputs_embeds)\n",
    "#             logits = outputs.logits[:, -1, :]\n",
    "#             next_token = torch.argmax(logits, dim=-1)\n",
    "\n",
    "#             # Stop if the <end> token is generated\n",
    "#             if next_token.item() == gpt2_tokenizer.eos_token_id:\n",
    "#                 break\n",
    "\n",
    "#             # Append new token\n",
    "#             input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)\n",
    "\n",
    "#     # Decode and clean the caption\n",
    "#     caption = gpt2_tokenizer.decode(input_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "    \n",
    "#     # **Fix: Stop at the first <end> token and remove redundant <start>**\n",
    "#     caption = caption.split(\"<end>\")[0].strip()  # Keep only the first valid caption\n",
    "#     caption = caption.replace(\"<start>\", \"\").strip()  # Remove <start> token if present\n",
    "\n",
    "#     # **Fix: Prevent redundant OCR text in the final output**\n",
    "#     if ocr_text.strip().lower() in caption.lower():\n",
    "#         final_caption = caption\n",
    "#     else:\n",
    "#         final_caption = f\"Detected text: {ocr_text}. {caption}\"\n",
    "\n",
    "#     return final_caption\n",
    "\n",
    "import re  # Import regex for OCR text cleanup\n",
    "\n",
    "def clean_ocr_text(ocr_text):\n",
    "    \"\"\"Removes noisy characters and keeps only readable words.\"\"\"\n",
    "    ocr_text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", ocr_text)  # Remove symbols\n",
    "    words = ocr_text.split()\n",
    "    return \" \".join(words[:5])  # Keep only the first 6 words for brevity\n",
    "\n",
    "def generate_caption(image_path, model, clip_model, gpt2_tokenizer, device, max_length=50):\n",
    "    \"\"\"Generate image caption using CLIP-GPT2 with OCR conditioning (removes repetition & filters OCR).\"\"\"\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocessed_image = clip_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "\n",
    "    # Extract CLIP features\n",
    "    with torch.no_grad():\n",
    "        clip_features = clip_model.get_image_features(preprocessed_image).float()\n",
    "\n",
    "    # Extract OCR text\n",
    "    ocr_text = get_ocr_text(image_path)\n",
    "    print(f\"[OCR]: {ocr_text}\")\n",
    "\n",
    "    #  **Clean the OCR text**  \n",
    "    clean_text = clean_ocr_text(ocr_text)\n",
    "\n",
    "    #  **New Prompt Style**  \n",
    "    if clean_text.strip():\n",
    "        prompt = f\"Text detected: {clean_text}. Describe the scene:\"\n",
    "    else:\n",
    "        prompt = \"Describe the scene:\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    input_ids = gpt2_tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "    # Project CLIP features\n",
    "    prefix = model.clip_project(clip_features).view(1, model.prefix_length, model.gpt_embedding_size)\n",
    "\n",
    "    # Generate caption\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Embed input tokens\n",
    "            gpt2_embeddings = model.gpt.transformer.wte(input_ids)\n",
    "            inputs_embeds = torch.cat((prefix, gpt2_embeddings), dim=1)\n",
    "\n",
    "            # Generate logits\n",
    "            outputs = model.gpt(inputs_embeds=inputs_embeds)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Stop if the <end> token is generated\n",
    "            if next_token.item() == gpt2_tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            # Append new token\n",
    "            input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)\n",
    "\n",
    "    # Decode and clean the caption\n",
    "    caption = gpt2_tokenizer.decode(input_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "\n",
    "    #  **Remove repetition**  \n",
    "    unique_sentences = list(dict.fromkeys(caption.split(\". \")))  # Remove duplicate sentences\n",
    "    caption = \". \".join(unique_sentences)  # Reconstruct without repetition\n",
    "\n",
    "    #  **Clean up and finalize the caption**  \n",
    "    caption = caption.replace(\"Describe the scene:\", \"\").strip()\n",
    "    caption = caption.replace(\"Text detected:\", \"\").strip()\n",
    "    caption = caption.replace(\"<start>\", \"\").strip()\n",
    "    caption = caption.split(\"<end>\")[0].strip()  # Stop at first `<end>`\n",
    "\n",
    "    #  **Append cleaned OCR text only if needed**  \n",
    "    if clean_text.strip().lower() not in caption.lower():\n",
    "        final_caption = f\"{caption}. (Detected text: {clean_text})\"\n",
    "    else:\n",
    "        final_caption = caption  # No need to re-add OCR text\n",
    "\n",
    "    return final_caption\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "image_path = r\"D:\\vidisha\\vizwiz\\real time test imgs\\anvi.jpg\"\n",
    "caption = generate_caption(image_path, model, clip_model, gpt2_tokenizer, device)\n",
    "print(f\"Final Caption: {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2e1e4345-57ea-49e1-89b1-4c357513295f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vdsha\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\easyocr\\detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
      "C:\\Users\\vdsha\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\easyocr\\recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OCR]: Smile\n",
      "Final Caption: Smile. A woman wearing a pink t-shirt with a blue flower on it.\n"
     ]
    }
   ],
   "source": [
    "#improving second approach of prompt tuning for gpt2\n",
    "import re\n",
    "import torch\n",
    "\n",
    "def clean_ocr_text(ocr_text):\n",
    "    \"\"\"Removes noisy characters and keeps only readable words.\"\"\"\n",
    "    ocr_text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", ocr_text)  # Remove symbols\n",
    "    words = ocr_text.split()\n",
    "    return \" \".join(words[:10])  # Keep only the first 5 words for brevity\n",
    "\n",
    "def remove_hallucinations(caption, image_objects):\n",
    "    \"\"\"Removes hallucinated objects that aren't detected by CLIP.\"\"\"\n",
    "    hallucinated_objects = [\"bookcase\", \"water\", \"person\", \"chair\", \"sofa\", \"lamp\"]  # Common hallucinations\n",
    "    filtered_caption = \" \".join([word for word in caption.split() if word.lower() not in hallucinated_objects])\n",
    "    return filtered_caption.strip()\n",
    "\n",
    "def generate_caption(image_path, model, clip_model, gpt2_tokenizer, device, max_length=50):\n",
    "    \"\"\"Generate image caption using CLIP-GPT2 with OCR conditioning (prevents hallucination).\"\"\"\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocessed_image = clip_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "\n",
    "    # Extract CLIP features\n",
    "    with torch.no_grad():\n",
    "        clip_features = clip_model.get_image_features(preprocessed_image).float()\n",
    "\n",
    "    # Extract and clean OCR text\n",
    "    ocr_text = get_ocr_text(image_path)\n",
    "    clean_text = clean_ocr_text(ocr_text)\n",
    "    print(f\"[OCR]: {clean_text}\")\n",
    "\n",
    "    #  **Better OCR Integration**\n",
    "    if clean_text.strip():\n",
    "        prompt = f\"Image contains text: '{clean_text}'. What else is in the image?\"\n",
    "    else:\n",
    "        prompt = \"What do you see in the image?\"\n",
    "\n",
    "    # Tokenize the refined prompt\n",
    "    input_ids = gpt2_tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "    # Project CLIP features\n",
    "    prefix = model.clip_project(clip_features).view(1, model.prefix_length, model.gpt_embedding_size)\n",
    "\n",
    "    # Generate caption\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Embed input tokens\n",
    "            gpt2_embeddings = model.gpt.transformer.wte(input_ids)\n",
    "            inputs_embeds = torch.cat((prefix, gpt2_embeddings), dim=1)\n",
    "\n",
    "            # Generate logits\n",
    "            outputs = model.gpt(inputs_embeds=inputs_embeds)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Stop at the first `<eos>` token\n",
    "            if next_token.item() == gpt2_tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            # Append new token\n",
    "            input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)\n",
    "\n",
    "    # Decode and clean the caption\n",
    "    caption = gpt2_tokenizer.decode(input_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "\n",
    "    #  **Final Cleanup**\n",
    "    caption = caption.replace(prompt, \"\").strip()  # Remove any leftover prompt text\n",
    "    caption = caption.replace(\"<start>\", \"\").strip()\n",
    "    caption = caption.split(\"<end>\")[0].strip()  # Keep text before first `<end>`\n",
    "\n",
    "    #  **Hallucination Removal**\n",
    "    caption = remove_hallucinations(caption, clip_features)\n",
    "\n",
    "    #  **Ensure OCR text is naturally included**\n",
    "    if clean_text.strip().lower() not in caption.lower():\n",
    "        caption = f\"{clean_text}. {caption}\"\n",
    "\n",
    "    return caption\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "image_path = r\"D:\\vidisha\\vizwiz\\real time test imgs\\anvi.jpg\"\n",
    "caption = generate_caption(image_path, model, clip_model, gpt2_tokenizer, device)\n",
    "print(f\"Final Caption: {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c75bc133-867b-4939-8576-6ee1cf3d8d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Features Shape: torch.Size([1, 512])\n",
      "CLIP Features Sample: tensor([-0.2584, -0.0551,  0.0754, -0.0036, -0.0451], device='cuda:0')\n",
      "Prefix Shape: torch.Size([1, 10, 768])\n",
      "Prefix Sample: tensor([-0.0038, -0.1391,  0.2918,  0.1991,  0.0606], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Tokens: tensor([[50256,     0,     2,     0,     1,     3,     0,    20,     1,     5,\n",
      "             1,     0,     3,     0,     0,    21,     7,     0,     4,     0,\n",
      "             0,     0,     0,     1,     1,     2,     0,    11,     0,     0,\n",
      "             3,     0,     5,     3,     0,     9,     0,     2,     0,    12,\n",
      "             0,     0,     0,    13,     4,     0,     0,     0,     0,     0,\n",
      "             0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"CLIP Features Shape:\", clip_features.shape)\n",
    "print(\"CLIP Features Sample:\", clip_features[0, :5])  # Print first 5 values\n",
    "prefix = model.clip_project(clip_features).view(1, model.prefix_length, model.gpt_embedding_size)\n",
    "print(\"Prefix Shape:\", prefix.shape)\n",
    "print(\"Prefix Sample:\", prefix[0, 0, :5])  # Print first 5 values of the first prefix\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d533bd0-cb1b-4e45-aa6c-e0b52ff44c98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_env)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
